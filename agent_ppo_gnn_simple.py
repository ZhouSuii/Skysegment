# ç®€åŒ–ç‰ˆ GNN-PPOï¼šç§»é™¤å¤æ‚è®¾è®¡ï¼Œä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torch.distributions import Categorical

# === å›æ»šåˆ°ç®€åŒ–çš„GCNå®ç° ===
class SimpleGCNConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(SimpleGCNConv, self).__init__()
        self.linear = nn.Linear(in_channels, out_channels)
        
    def forward(self, x, edge_index):
        num_nodes = x.size(0)
        
        # æ„å»ºé‚»æ¥çŸ©é˜µï¼ˆåŸå§‹ç®€æ´å®ç°ï¼‰
        adj = torch.zeros(num_nodes, num_nodes, device=x.device)
        if edge_index.size(1) > 0:  # æ£€æŸ¥æ˜¯å¦æœ‰è¾¹
            adj[edge_index[0], edge_index[1]] = 1.0
        
        # æ·»åŠ è‡ªè¿æ¥
        adj += torch.eye(num_nodes, device=x.device)
        
        # åº¦å½’ä¸€åŒ–
        degree = adj.sum(dim=1, keepdim=True)
        degree = torch.where(degree > 0, degree, torch.ones_like(degree))
        adj = adj / degree
        
        # å›¾å·ç§¯ï¼šA * X * W
        out = torch.mm(adj, x)
        out = self.linear(out)
        return out


# === ä¼˜åŒ–åçš„GNNç­–ç•¥ç½‘ç»œï¼šæ”¯æŒçœŸæ­£çš„æ‰¹é‡å¤„ç† ===
class SimplePPOPolicyGNN(nn.Module):
    def __init__(self, node_feature_dim, action_size, hidden_dim=64):
        super(SimplePPOPolicyGNN, self).__init__()
        self.action_size = action_size
        self.hidden_dim = hidden_dim
        
        # === è¶…ç®€åŒ–ï¼šåªç”¨1å±‚GNN ===
        self.gnn = SimpleGCNConv(node_feature_dim, hidden_dim)
        
        # === ç›´æ¥åŠ¨ä½œé¢„æµ‹ï¼ˆç§»é™¤åŒå¤´è®¾è®¡ï¼‰===
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_size),
            nn.Softmax(dim=-1)
        )
        
        # === ç®€åŒ–çš„Critic ===
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

        # === ä¼˜åŒ–ï¼šæ”¹è¿›æƒé‡åˆå§‹åŒ– ===
        self._init_weights()

    def _init_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # Heåˆå§‹åŒ–é€‚ç”¨äºReLUæ¿€æ´»
                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

    def forward(self, graph_data):
        x = graph_data['node_features']
        edge_index = graph_data['edge_index']
        
        # å•å±‚GNNç‰¹å¾æå–
        x = F.relu(self.gnn(x, edge_index))
        
        # å…¨å±€å›¾è¡¨ç¤ºï¼šç®€å•å¹³å‡æ± åŒ–
        graph_repr = torch.mean(x, dim=0, keepdim=True)
        
        # åŠ¨ä½œæ¦‚ç‡å’Œä»·å€¼
        action_probs = self.actor(graph_repr).squeeze(0)  # [action_size]
        value = self.critic(graph_repr).squeeze()  # scalar
        
        return action_probs, value

    def forward_batch(self, batch_graph_data_list):
        """=== æ–°å¢ï¼šçœŸæ­£çš„æ‰¹é‡å‰å‘ä¼ æ’­ ==="""
        batch_size = len(batch_graph_data_list)
        device = next(self.parameters()).device
        
        # æ‰¹é‡å¤„ç†æ¯ä¸ªå›¾
        batch_action_probs = []
        batch_values = []
        
        for graph_data in batch_graph_data_list:
            # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if isinstance(graph_data['node_features'], np.ndarray):
                x = torch.tensor(graph_data['node_features'], dtype=torch.float32, device=device)
            else:
                x = graph_data['node_features'].to(device)
                
            if isinstance(graph_data['edge_index'], np.ndarray):
                edge_index = torch.tensor(graph_data['edge_index'], dtype=torch.long, device=device)
            else:
                edge_index = graph_data['edge_index'].to(device)
            
            # GNNç‰¹å¾æå–
            x = F.relu(self.gnn(x, edge_index))
            graph_repr = torch.mean(x, dim=0, keepdim=True)
            
            # åŠ¨ä½œæ¦‚ç‡å’Œä»·å€¼
            action_probs = self.actor(graph_repr).squeeze(0)
            value = self.critic(graph_repr).squeeze()
            
            batch_action_probs.append(action_probs)
            batch_values.append(value)
        
        # å †å ç»“æœ
        batch_action_probs = torch.stack(batch_action_probs)  # [batch_size, action_size]
        batch_values = torch.stack(batch_values)  # [batch_size]
        
        return batch_action_probs, batch_values

    def evaluate_batch(self, batch_graph_data_list, batch_actions):
        """=== æ–°å¢ï¼šæ‰¹é‡è¯„ä¼° ==="""
        batch_action_probs, batch_values = self.forward_batch(batch_graph_data_list)
        
        # è®¡ç®—åŠ¨ä½œå¯¹æ•°æ¦‚ç‡
        action_dists = Categorical(batch_action_probs)
        action_log_probs = action_dists.log_prob(batch_actions)
        
        # è®¡ç®—ç†µ
        entropies = action_dists.entropy()
        
        return action_log_probs, batch_values, entropies

    def act(self, graph_data):
        action_probs, _ = self.forward(graph_data)
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob

    def evaluate(self, graph_data, action):
        action_probs, value = self.forward(graph_data)
        dist = Categorical(action_probs)
        action_log_prob = dist.log_prob(action)
        entropy = dist.entropy()
        return action_log_prob, value, entropy


# === å¤§å¹…ä¼˜åŒ–çš„PPOæ™ºèƒ½ä½“ï¼šçœŸæ­£çš„æ‰¹é‡å¤„ç† ===
class SimplePPOAgentGNN:
    def __init__(self, state_size, action_size, config=None):
        self.state_size = state_size
        self.action_size = action_size
        
        if config is None:
            config = {}

        # === æ›´ä¿å®ˆçš„è¶…å‚æ•° ===
        self.gamma = config.get('gamma', 0.99)
        self.gae_lambda = config.get('gae_lambda', 0.95)
        self.clip_ratio = config.get('clip_ratio', 0.2)
        self.learning_rate = config.get('learning_rate', 0.0003)  # é€‚ä¸­çš„å­¦ä¹ ç‡
        self.ppo_epochs = config.get('ppo_epochs', 4)  # æ ‡å‡†PPOè½®æ•°
        self.batch_size = config.get('batch_size', 128)  # å¢å¤§æ‰¹é‡
        self.entropy_coef = config.get('entropy_coef', 0.01)
        self.value_coef = config.get('value_coef', 0.5)
        self.update_frequency = config.get('update_frequency', 64)  # æ›´å¤§çš„æ›´æ–°é¢‘ç‡
        
        # ç¼“å†²åŒº
        self.memory_capacity = config.get('memory_capacity', 20000)
        self.graph_data_buffer = []
        self.actions = np.zeros(self.memory_capacity, dtype=np.int64)
        self.log_probs = np.zeros(self.memory_capacity, dtype=np.float32)
        self.rewards = np.zeros(self.memory_capacity, dtype=np.float32)
        self.dones = np.zeros(self.memory_capacity, dtype=np.float32)
        self.values = np.zeros(self.memory_capacity, dtype=np.float32)
        
        self.buffer_ptr = 0
        self.traj_start_ptr = 0
        
        # è®¾å¤‡è®¾ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ ä¼˜åŒ–ç®€åŒ–PPO-GNNä½¿ç”¨è®¾å¤‡: {self.device}")

        # === ç®€åŒ–çš„ç½‘ç»œ ===
        self.policy = SimplePPOPolicyGNN(
            node_feature_dim=state_size,
            action_size=action_size,
            hidden_dim=config.get('hidden_dim', 128)  # å¢åŠ éšè—å±‚å¤§å°
        ).to(self.device)
        
        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.learning_rate)

        # tensorboard
        from tensorboard_logger import TensorboardLogger
        tensorboard_config = config.get('tensorboard_config', {})
        self.use_tensorboard = config.get('use_tensorboard', True)
        if self.use_tensorboard:
            self.logger = TensorboardLogger(tensorboard_config)
        else:
            self.logger = None

        # === æ–°å¢ï¼šæ‰¹é‡å¤„ç†ä¼˜åŒ– ===
        self.enable_batch_processing = config.get('enable_batch_processing', True)
        print(f"ğŸ”§ æ‰¹é‡å¤„ç†: {'å¯ç”¨' if self.enable_batch_processing else 'ç¦ç”¨'}")

    def _move_graph_to_device(self, graph_data, target_device=None):
        if target_device is None:
            target_device = self.device
        
        moved_data = {}
        for key, value in graph_data.items():
            if isinstance(value, torch.Tensor):
                moved_data[key] = value.to(target_device)
            elif isinstance(value, np.ndarray):
                moved_data[key] = torch.tensor(value).to(target_device)
            else:
                moved_data[key] = value
        return moved_data

    def act(self, graph_data):
        graph_data = self._move_graph_to_device(graph_data)
        
        self.policy.eval()
        with torch.no_grad():
            action, log_prob = self.policy.act(graph_data)
            _, value = self.policy(graph_data)
        self.policy.train()

        # å­˜å‚¨åˆ°ç¼“å†²åŒº
        if self.buffer_ptr < self.memory_capacity:
            cpu_graph_data = self._move_graph_to_device(graph_data, target_device='cpu')
            self.graph_data_buffer.append(cpu_graph_data)
            
            self.actions[self.buffer_ptr] = action
            self.log_probs[self.buffer_ptr] = log_prob.item() if isinstance(log_prob, torch.Tensor) else log_prob
            self.values[self.buffer_ptr] = value.item()
        else:
            print("Warning: ç®€åŒ–PPO-GNN buffer overflow!")
            self.buffer_ptr = 0
            self.graph_data_buffer = []
            cpu_graph_data = self._move_graph_to_device(graph_data, target_device='cpu')
            self.graph_data_buffer.append(cpu_graph_data)
            
            self.actions[self.buffer_ptr] = action
            self.log_probs[self.buffer_ptr] = log_prob.item() if isinstance(log_prob, torch.Tensor) else log_prob
            self.values[self.buffer_ptr] = value.item()

        return action

    def store_transition(self, reward, done):
        if self.buffer_ptr < self.memory_capacity:
            self.rewards[self.buffer_ptr] = reward
            self.dones[self.buffer_ptr] = float(done)
            self.buffer_ptr += 1

    def update(self):
        steps_collected = self.buffer_ptr - self.traj_start_ptr
        
        if steps_collected < self.update_frequency and self.buffer_ptr < self.memory_capacity:
            return 0.0
            
        if steps_collected <= 0:
            self.traj_start_ptr = self.buffer_ptr
            return 0.0

        # æ•°æ®å‡†å¤‡
        indices = np.arange(self.traj_start_ptr, self.buffer_ptr)
        graph_data_list = [self.graph_data_buffer[i] for i in indices]
        actions_np = self.actions[indices]
        old_log_probs_np = self.log_probs[indices]
        rewards_np = self.rewards[indices]
        dones_np = self.dones[indices]
        values_np = self.values[indices]

        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        returns, advantages = self._compute_returns_advantages_vectorized(rewards_np, dones_np, values_np)

        # è½¬ç§»åˆ°GPU
        actions = torch.tensor(actions_np, dtype=torch.long).to(self.device)
        old_log_probs = torch.tensor(old_log_probs_np, dtype=torch.float32).to(self.device)
        returns_tensor = torch.tensor(returns, dtype=torch.float32).to(self.device)
        advantages_tensor = torch.tensor(advantages, dtype=torch.float32).to(self.device)

        # === æ ¸å¿ƒä¼˜åŒ–ï¼šçœŸæ­£çš„æ‰¹é‡PPOæ›´æ–° ===
        total_loss = 0.0
        update_rounds = 0
        dataset_size = len(indices)
        current_batch_size = min(self.batch_size, dataset_size)

        for epoch in range(self.ppo_epochs):
            perm_indices = np.random.permutation(dataset_size)
            
            for start_idx in range(0, dataset_size, current_batch_size):
                end_idx = min(start_idx + current_batch_size, dataset_size)
                batch_indices = perm_indices[start_idx:end_idx]
                
                # === å…³é”®ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†è€Œä¸æ˜¯é€ä¸ªå¤„ç† ===
                if self.enable_batch_processing and len(batch_indices) > 1:
                    # ä½¿ç”¨çœŸæ­£çš„æ‰¹é‡å¤„ç†
                    batch_graph_data_list = [graph_data_list[i] for i in batch_indices]
                    batch_actions = actions[batch_indices]
                    batch_old_log_probs = old_log_probs[batch_indices]
                    batch_returns = returns_tensor[batch_indices]
                    batch_advantages = advantages_tensor[batch_indices]
                    
                    # æ‰¹é‡è¯„ä¼°
                    new_log_probs, values, entropies = self.policy.evaluate_batch(
                        batch_graph_data_list, batch_actions
                    )
                    
                    # PPOæŸå¤±è®¡ç®—ï¼ˆå‘é‡åŒ–ï¼‰
                    ratios = torch.exp(new_log_probs - batch_old_log_probs)
                    surr1 = ratios * batch_advantages
                    surr2 = torch.clamp(ratios, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * batch_advantages
                    policy_loss = -torch.min(surr1, surr2).mean()
                    
                    value_loss = F.mse_loss(values, batch_returns)
                    entropy_loss = -entropies.mean()
                    
                    loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss
                    
                    # å•æ¬¡åå‘ä¼ æ’­
                    self.optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)
                    self.optimizer.step()
                    
                    total_loss += loss.item()
                    update_rounds += 1
                else:
                    # å›é€€åˆ°é€ä¸ªå¤„ç†ï¼ˆå°æ‰¹é‡æ—¶ï¼‰
                    batch_loss = 0.0
                    for i in batch_indices:
                        graph_data = self._move_graph_to_device(graph_data_list[i])
                        
                        new_log_prob, value, entropy = self.policy.evaluate(graph_data, actions[i])

                        # PPOæŸå¤±è®¡ç®—
                        ratio = torch.exp(new_log_prob - old_log_probs[i])
                        surr1 = ratio * advantages_tensor[i]
                        surr2 = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages_tensor[i]
                        policy_loss = -torch.min(surr1, surr2)

                        value_loss = F.mse_loss(value, returns_tensor[i])
                        entropy_loss = -entropy

                        loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss
                        batch_loss += loss

                    # å¹³å‡æ‰¹æ¬¡æŸå¤±å¹¶åå‘ä¼ æ’­
                    avg_batch_loss = batch_loss / len(batch_indices)
                    total_loss += avg_batch_loss.item()

                    self.optimizer.zero_grad()
                    avg_batch_loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)
                    self.optimizer.step()

                    update_rounds += 1

        # æ¸…ç†
        self.traj_start_ptr = self.buffer_ptr
        if self.buffer_ptr == self.memory_capacity:
            self.traj_start_ptr = 0
            self.buffer_ptr = 0
            self.graph_data_buffer = []

        return total_loss / max(1, update_rounds)

    def _compute_returns_advantages_vectorized(self, rewards, dones, values):
        """è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿"""
        if dones[-1]:
            next_value = 0.0
        else:
            with torch.no_grad():
                last_state_idx = self.buffer_ptr - 1
                if last_state_idx >= 0 and last_state_idx < len(self.graph_data_buffer):
                    last_graph_data = self._move_graph_to_device(self.graph_data_buffer[last_state_idx])
                    _, value = self.policy(last_graph_data)
                    next_value = value.item()
                else:
                    next_value = 0.0
        
        values_with_next = np.append(values, next_value)
        advantages = np.zeros_like(rewards, dtype=np.float32)
        
        gae = 0.0
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + self.gamma * values_with_next[t + 1] * (1 - dones[t]) - values_with_next[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages[t] = gae
            
        returns = advantages + values
        
        if len(advantages) > 1:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        return returns, advantages

    def save_model(self, filepath):
        torch.save(self.policy.state_dict(), filepath)
        if self.logger is not None:
            self.logger.close()

    def load_model(self, filepath):
        self.policy.load_state_dict(torch.load(filepath, map_location=self.device))
        
    def eval_mode(self):
        self.policy.eval()
        
    def train_mode(self):
        self.policy.train() 