# GNN-PPO 改进总结

## 问题诊断

分析了GNN-PPO在图分区任务上性能不佳的原因，主要发现以下问题：

1. **训练不稳定**：奖励、损失函数剧烈波动，难以收敛
2. **梯度问题严重**：存在梯度爆炸现象，日志中多次出现"Large gradient norm"警告
3. **嵌入层表现异常**：GCN层和Actor/Critic输出的嵌入范数持续增大
4. **性能指标差**：weight_variance、edge_cut、modularity等关键图分区指标远逊于其他基线算法

## 改进措施

### 1. 梯度控制优化

- **动态梯度裁剪**：将梯度裁剪从固定值改为可配置参数`max_grad_norm`，默认设置为0.5
- **梯度监控**：增强了梯度异常检测，添加了梯度范数的记录和警告
- **TensorBoard记录**：将梯度范数记录到TensorBoard，方便可视化分析

### 2. 网络架构改进

- **层归一化（LayerNorm）**：在GCN层之后和全连接层之间添加了LayerNorm，有助于：
  - 提高训练稳定性
  - 避免特征分布偏移
  - 减轻梯度消失/爆炸问题
  - 提高优化速度
- **优化残差连接**：确保残差连接在Layer Norm之后，保持信号干净

### 3. 自适应学习优化

- **动态学习率调度**：实现了`ReduceLROnPlateau`学习率调度器，当训练停滞时自动降低学习率
- **学习率监控**：将学习率变化记录到TensorBoard，并定期打印学习率变化

### 4. 归一化策略增强

- **奖励归一化**：实现了滑动窗口奖励归一化，使用指数移动平均（EMA）跟踪奖励分布
  - 使训练对奖励幅度不敏感
  - 降低奖励波动对训练的影响
  - 提供可配置的缩放系数控制奖励强度
- **优势函数归一化**：保留并增强了优势函数的标准化，提高策略梯度估计的准确性

## 如何使用新参数

要使用这些新功能，可以在创建GNN-PPO智能体时通过`config`字典进行配置：

```python
config = {
    # 梯度控制
    'max_grad_norm': 0.5,  # 调整梯度裁剪阈值，越小越保守
    
    # 学习率调度器
    'use_lr_scheduler': True,
    'lr_scheduler_factor': 0.5,    # 学习率下降因子
    'lr_scheduler_patience': 10,   # 容忍多少个episode没有进步
    'lr_scheduler_min': 1e-6,      # 最小学习率
    
    # 奖励归一化
    'use_reward_norm': True,       # 是否启用奖励归一化
    'reward_norm_scale': 1.0,      # 归一化奖励的缩放系数
    'reward_ema_factor': 0.99,     # 奖励统计的指数移动平均因子
    
    # 原有参数
    'learning_rate': 0.0003,       # 初始学习率，建议降低到0.0003或更小
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'clip_ratio': 0.2,
    'entropy_coef': 0.01,
    'value_coef': 0.5,
    'hidden_dim': 256,
    'adam_beta1': 0.9,
    'adam_beta2': 0.999
}
```

## 调参建议

1. **首先平衡训练**：
   - 从较小的学习率开始（1e-4 ~ 3e-4）
   - 设置适当的梯度裁剪阈值（0.5 ~ 1.0）
   - 开启所有归一化选项
   
2. **在训练稳定后**：
   - 逐步增加`hidden_dim`提高模型容量
   - 微调`entropy_coef`平衡探索与利用
   - 适当调整`value_coef`平衡策略与价值学习
   
3. **对于不同规模的图**：
   - 小图（<50节点）：可以使用更大的`batch_size`（32~64）
   - 大图（>100节点）：减小`batch_size`（8~16）并增加`n_steps`

## 监控要点

训练过程中应特别关注以下指标：

1. **梯度范数**：持续监控梯度范数，如果频繁出现超过10的范数，考虑降低学习率或增加裁剪强度
2. **嵌入统计**：观察GCN层输出的均值、标准差和范数，数值过大表明可能有训练稳定性问题
3. **奖励均值**：观察原始奖励和归一化奖励的趋势，稳定上升是良好的信号
4. **学习率变化**：学习率快速下降表明训练可能陷入局部最优，需要尝试不同的初始化或超参数

## 预期效果

经过这些优化，GNN-PPO应能改善以下方面：

1. **训练稳定性**：损失和奖励波动应显著减小
2. **收敛速度**：应能更快地收敛到更好的策略
3. **泛化能力**：对不同大小和结构的图应有更好的泛化能力
4. **分区质量**：modularity、edge_cut等关键指标应有改善

建议以更加渐进的方式调参，每次只修改一两个参数，观察其效果后再进行下一步调整。
