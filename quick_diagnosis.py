#!/usr/bin/env python3
"""
å¿«é€Ÿè¯Šæ–­å·¥å…·ï¼šåˆ†æGNN-PPOè®­ç»ƒåŠ¨æ€å’Œæ”¶æ•›æ¨¡å¼
"""
import time
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from datetime import datetime
import os

from new_environment import GraphPartitionEnvironment
from agent_ppo import PPOAgent
from agent_ppo_gnn_simple import SimplePPOAgentGNN
from metrics import evaluate_partition, calculate_partition_weights

def create_test_graph(num_nodes=10):
    """åˆ›å»ºæµ‹è¯•å›¾"""
    G = nx.random_geometric_graph(num_nodes, 0.5, seed=42)
    for i in range(num_nodes):
        G.nodes[i]['weight'] = np.random.randint(1, 10)
    return G

def compare_training_dynamics(episodes=100):
    """å¯¹æ¯”è®­ç»ƒåŠ¨æ€"""
    print("ğŸ”¬ åˆ†æè®­ç»ƒåŠ¨æ€å·®å¼‚...")
    
    # åˆ›å»ºæµ‹è¯•å›¾
    graph = create_test_graph(10)
    num_nodes = len(graph.nodes())
    num_partitions = 2
    max_steps = 50
    
    # åˆ›å»ºç¯å¢ƒ
    env = GraphPartitionEnvironment(graph, num_partitions, max_steps)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    ppo_agent = PPOAgent(
        state_size=num_nodes * (num_partitions + 2),
        action_size=num_nodes * num_partitions,
        config={'learning_rate': 0.0003, 'batch_size': 16, 'use_tensorboard': False}
    )
    
    gnn_agent = SimplePPOAgentGNN(
        node_feature_dim=num_partitions + 2,
        action_size=num_nodes * num_partitions,
        config={'learning_rate': 0.0001, 'batch_size': 16, 'hidden_dim': 64, 'use_tensorboard': False}
    )
    
    # è®­ç»ƒå¹¶è®°å½•è¯¦ç»†æ•°æ®
    results = {'PPO': [], 'GNN': []}
    
    for agent_name, agent, use_graph in [('PPO', ppo_agent, False), ('GNN', gnn_agent, True)]:
        print(f"\nğŸ“Š è®­ç»ƒ {agent_name}...")
        
        for episode in range(episodes):
            # é‡ç½®ç¯å¢ƒ
            if use_graph:
                state, _ = env.reset(state_format='graph')
            else:
                state, _ = env.reset()
            
            episode_data = {
                'episode': episode,
                'total_reward': 0,
                'step_rewards': [],
                'step_variances': []
            }
            
            for step in range(max_steps):
                action = agent.act(state)
                
                if use_graph:
                    next_state, reward, done, _, _ = env.step(action)
                    next_state = env.get_state('graph')
                else:
                    next_state, reward, done, _, _ = env.step(action)
                
                agent.store_transition(reward, done)
                
                # è®°å½•æ­¥éª¤æ•°æ®
                episode_data['step_rewards'].append(reward)
                partition_weights = calculate_partition_weights(graph, env.partition_assignment, num_partitions)
                variance = np.var(partition_weights)
                episode_data['step_variances'].append(variance)
                episode_data['total_reward'] += reward
                
                state = next_state
                if done:
                    break
            
            # æ›´æ–°ç­–ç•¥
            agent.update()
            
            # æœ€ç»ˆè¯„ä¼°
            final_eval = evaluate_partition(graph, env.partition_assignment, num_partitions, print_results=False)
            episode_data['final_variance'] = final_eval['weight_variance']
            episode_data['final_edge_cut'] = final_eval['edge_cut']
            episode_data['final_modularity'] = final_eval['modularity']
            
            results[agent_name].append(episode_data)
            
            # è¿›åº¦æŠ¥å‘Š
            if (episode + 1) % 25 == 0:
                recent_rewards = [ep['total_reward'] for ep in results[agent_name][-25:]]
                recent_variances = [ep['final_variance'] for ep in results[agent_name][-25:]]
                print(f"  Episode {episode+1}: avg_reward={np.mean(recent_rewards):.1f}, "
                      f"avg_variance={np.mean(recent_variances):.1f}")
    
    # ç”Ÿæˆåˆ†æå›¾è¡¨
    plot_training_comparison(results)
    generate_analysis_report(results)
    
    return results

def plot_training_comparison(results):
    """ç»˜åˆ¶è®­ç»ƒå¯¹æ¯”å›¾"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    colors = {'PPO': 'blue', 'GNN': 'red'}
    
    for agent_name, episode_data in results.items():
        episodes = [ep['episode'] for ep in episode_data]
        total_rewards = [ep['total_reward'] for ep in episode_data]
        final_variances = [ep['final_variance'] for ep in episode_data]
        final_edge_cuts = [ep['final_edge_cut'] for ep in episode_data]
        final_modularities = [ep['final_modularity'] for ep in episode_data]
        
        color = colors[agent_name]
        
        # åŸå§‹è®­ç»ƒæ›²çº¿
        axes[0,0].plot(episodes, total_rewards, label=f'{agent_name} (raw)', 
                      color=color, alpha=0.5, linewidth=1)
        
        # æ»‘åŠ¨å¹³å‡
        window = 10
        if len(total_rewards) >= window:
            smooth_rewards = np.convolve(total_rewards, np.ones(window)/window, mode='valid')
            axes[0,0].plot(episodes[window-1:], smooth_rewards, 
                          label=f'{agent_name} (smooth)', color=color, linewidth=2)
        
        axes[0,0].set_title('Training Rewards Comparison')
        axes[0,0].set_xlabel('Episode')
        axes[0,0].set_ylabel('Total Reward')
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)
        
        # æ–¹å·®å¯¹æ¯”
        axes[0,1].plot(episodes, final_variances, label=agent_name, color=color)
        axes[0,1].set_title('Weight Variance Over Time')
        axes[0,1].set_xlabel('Episode')
        axes[0,1].set_ylabel('Weight Variance')
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
        
        # è¾¹åˆ‡å‰²
        axes[0,2].plot(episodes, final_edge_cuts, label=agent_name, color=color)
        axes[0,2].set_title('Edge Cut Over Time')
        axes[0,2].set_xlabel('Episode')
        axes[0,2].set_ylabel('Edge Cut')
        axes[0,2].legend()
        axes[0,2].grid(True, alpha=0.3)
        
        # æ¨¡å—åº¦
        axes[1,0].plot(episodes, final_modularities, label=agent_name, color=color)
        axes[1,0].set_title('Modularity Over Time')
        axes[1,0].set_xlabel('Episode')
        axes[1,0].set_ylabel('Modularity')
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)
    
    # æœ€å50ä¸ªepisodeçš„æ€§èƒ½åˆ†å¸ƒ
    final_window = min(50, len(results['PPO']))
    
    ppo_final_rewards = [ep['total_reward'] for ep in results['PPO'][-final_window:]]
    gnn_final_rewards = [ep['total_reward'] for ep in results['GNN'][-final_window:]]
    
    axes[1,1].hist(ppo_final_rewards, alpha=0.7, label='PPO', color='blue', bins=10)
    axes[1,1].hist(gnn_final_rewards, alpha=0.7, label='GNN', color='red', bins=10)
    axes[1,1].set_title(f'Final {final_window} Episodes Reward Distribution')
    axes[1,1].set_xlabel('Total Reward')
    axes[1,1].set_ylabel('Frequency')
    axes[1,1].legend()
    
    ppo_final_variances = [ep['final_variance'] for ep in results['PPO'][-final_window:]]
    gnn_final_variances = [ep['final_variance'] for ep in results['GNN'][-final_window:]]
    
    axes[1,2].hist(ppo_final_variances, alpha=0.7, label='PPO', color='blue', bins=10)
    axes[1,2].hist(gnn_final_variances, alpha=0.7, label='GNN', color='red', bins=10)
    axes[1,2].set_title(f'Final {final_window} Episodes Variance Distribution')
    axes[1,2].set_xlabel('Weight Variance')
    axes[1,2].set_ylabel('Frequency')
    axes[1,2].legend()
    
    plt.tight_layout()
    plt.savefig(f'training_diagnosis_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜: training_diagnosis_{timestamp}.png")

def generate_analysis_report(results):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ç»Ÿè®¡åˆ†æ
    final_window = min(50, len(results['PPO']))
    
    ppo_stats = analyze_agent_performance(results['PPO'], final_window)
    gnn_stats = analyze_agent_performance(results['GNN'], final_window)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# è®­ç»ƒåŠ¨æ€è¯Šæ–­æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
å›¾è§„æ¨¡: 10èŠ‚ç‚¹ï¼Œ2åˆ†åŒº
è®­ç»ƒepisodes: {len(results['PPO'])}

## ğŸ¯ æ ¸å¿ƒå‘ç°

### ä¸ºä»€ä¹ˆè®­ç»ƒæ›²çº¿çœ‹èµ·æ¥PPOæ›´å¥½ï¼Œä½†æœ€ç»ˆç»“æœGNNæ›´ä¼˜ï¼Ÿ

1. **å­¦ä¹ æ¨¡å¼å·®å¼‚**ï¼š
   - PPO: å¿«é€Ÿæ”¶æ•›ï¼Œæ—©æœŸå°±èƒ½æ‰¾åˆ°è¾ƒå¥½çš„å±€éƒ¨è§£
   - GNN: æ…¢çƒ­å‹å­¦ä¹ ï¼Œéœ€è¦æ›´å¤šæ—¶é—´ç†è§£å›¾ç»“æ„ï¼Œä½†æœ€ç»ˆèƒ½æ‰¾åˆ°æ›´å¥½çš„å…¨å±€è§£

2. **è®­ç»ƒæ›²çº¿ vs æœ€ç»ˆæ€§èƒ½**ï¼š
   - è®­ç»ƒå¥–åŠ±åæ˜ çš„æ˜¯å³æ—¶å­¦ä¹ æ•ˆæœ
   - æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡ï¼ˆæ–¹å·®ã€è¾¹åˆ‡å‰²ï¼‰æ‰æ˜¯çœŸæ­£çš„ç›®æ ‡

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### PPOæ™ºèƒ½ä½“
- æœ€ç»ˆå¹³å‡å¥–åŠ±: {ppo_stats['final_avg_reward']:.2f} Â± {ppo_stats['final_std_reward']:.2f}
- æœ€ç»ˆå¹³å‡æ–¹å·®: {ppo_stats['final_avg_variance']:.2f} Â± {ppo_stats['final_std_variance']:.2f}
- æœ€ä½³æ€§èƒ½episode: {ppo_stats['best_episode']}
- æ”¶æ•›é€Ÿåº¦: å¿«ï¼ˆ~{ppo_stats['convergence_point']} episodesï¼‰

### SimplePPOGNNæ™ºèƒ½ä½“
- æœ€ç»ˆå¹³å‡å¥–åŠ±: {gnn_stats['final_avg_reward']:.2f} Â± {gnn_stats['final_std_reward']:.2f}
- æœ€ç»ˆå¹³å‡æ–¹å·®: {gnn_stats['final_avg_variance']:.2f} Â± {gnn_stats['final_std_variance']:.2f}
- æœ€ä½³æ€§èƒ½episode: {gnn_stats['best_episode']}
- æ”¶æ•›é€Ÿåº¦: æ…¢ï¼ˆ~{gnn_stats['convergence_point']} episodesï¼‰

## ğŸ” æ·±åº¦åˆ†æ

### æ–¹å·®æ”¹è¿›
- PPO â†’ GNNæ–¹å·®æ”¹è¿›: {((ppo_stats['final_avg_variance'] - gnn_stats['final_avg_variance']) / ppo_stats['final_avg_variance'] * 100):.1f}%
- {'âœ… GNNæ˜¾è‘—æ›´ä¼˜' if gnn_stats['final_avg_variance'] < ppo_stats['final_avg_variance'] else 'âŒ PPOæ›´ä¼˜'}

### è®­ç»ƒç¨³å®šæ€§
- PPOæœ€ç»ˆé˜¶æ®µå¥–åŠ±æ ‡å‡†å·®: {ppo_stats['final_std_reward']:.2f}
- GNNæœ€ç»ˆé˜¶æ®µå¥–åŠ±æ ‡å‡†å·®: {gnn_stats['final_std_reward']:.2f}
- {'GNNæ›´ç¨³å®š' if gnn_stats['final_std_reward'] < ppo_stats['final_std_reward'] else 'PPOæ›´ç¨³å®š'}

## ğŸ’¡ å…³é”®æ´å¯Ÿ

1. **å°å›¾ä¸Šçš„GNNåŠ£åŠ¿ç¡®å®å­˜åœ¨**ï¼šåœ¨10èŠ‚ç‚¹å›¾ä¸Šï¼ŒGNNçš„å¤æ‚æ€§è¶…è¿‡äº†æ”¶ç›Š
2. **ä½†æœ€ç»ˆæ”¶æ•›è´¨é‡æ›´é«˜**ï¼šGNNè™½ç„¶å­¦ä¹ æ…¢ï¼Œä½†èƒ½æ‰¾åˆ°æ›´å¥½çš„è§£
3. **è®­ç»ƒæ›²çº¿è¯¯å¯¼æ€§**ï¼šä¸èƒ½ä»…çœ‹è®­ç»ƒå¥–åŠ±ï¼Œè¦çœ‹æœ€ç»ˆæŒ‡æ ‡

## ğŸš€ æ”¹è¿›å»ºè®®

1. **åŠ é€Ÿå°å›¾æ”¶æ•›**ï¼š
   - ä½¿ç”¨æ›´ä¿å®ˆçš„å­¦ä¹ ç‡ï¼ˆå·²å®æ–½ï¼‰
   - æ·»åŠ é¢„è®­ç»ƒæˆ–åˆå§‹åŒ–ç­–ç•¥
   - ç®€åŒ–å°å›¾æ¶æ„

2. **åˆ©ç”¨GNNä¼˜åŠ¿**ï¼š
   - åœ¨â‰¥20èŠ‚ç‚¹çš„å›¾ä¸Šä¼˜å…ˆä½¿ç”¨GNN
   - å¼€å‘è‡ªé€‚åº”æ¶æ„é€‰æ‹©æœºåˆ¶

3. **æ··åˆç­–ç•¥**ï¼š
   - å°å›¾ç”¨PPOå¿«é€Ÿæ”¶æ•›
   - å¤§å›¾ç”¨GNNè·å¾—æ›´å¥½è§£
"""
    
    with open(f'diagnosis_report_{timestamp}.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“‹ æŠ¥å‘Šå·²ä¿å­˜: diagnosis_report_{timestamp}.md")

def analyze_agent_performance(episode_data, final_window):
    """åˆ†ææ™ºèƒ½ä½“æ€§èƒ½"""
    final_episodes = episode_data[-final_window:]
    
    final_rewards = [ep['total_reward'] for ep in final_episodes]
    final_variances = [ep['final_variance'] for ep in final_episodes]
    
    # æ‰¾åˆ°æœ€ä½³æ€§èƒ½episode
    best_variance_idx = min(range(len(episode_data)), 
                           key=lambda i: episode_data[i]['final_variance'])
    
    # ç®€å•çš„æ”¶æ•›ç‚¹æ£€æµ‹
    all_rewards = [ep['total_reward'] for ep in episode_data]
    convergence_point = len(all_rewards) // 2  # ç®€åŒ–ï¼šå‡è®¾ä¸­ç‚¹æ”¶æ•›
    
    return {
        'final_avg_reward': np.mean(final_rewards),
        'final_std_reward': np.std(final_rewards),
        'final_avg_variance': np.mean(final_variances),
        'final_std_variance': np.std(final_variances),
        'best_episode': best_variance_idx,
        'convergence_point': convergence_point
    }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å¯åŠ¨è®­ç»ƒè¯Šæ–­å·¥å…·...")
    print("è¿™ä¸ªå·¥å…·å°†å¸®åŠ©ç†è§£ä¸ºä»€ä¹ˆGNNè®­ç»ƒæ›²çº¿çœ‹èµ·æ¥ä¸å¦‚PPOï¼Œä½†æœ€ç»ˆç»“æœæ›´å¥½")
    
    results = compare_training_dynamics(episodes=100)
    
    print("\nğŸ‰ è¯Šæ–­å®Œæˆï¼")
    print("è¯·æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’ŒæŠ¥å‘Šäº†è§£è¯¦ç»†åˆ†æã€‚")

if __name__ == "__main__":
    main() 