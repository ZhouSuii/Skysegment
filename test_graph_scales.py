#!/usr/bin/env python3
"""
å¤šè§„æ¨¡å›¾æµ‹è¯•ï¼šéªŒè¯GNNåœ¨ä¸åŒå›¾è§„æ¨¡ä¸‹çš„ä¼˜åŠ¿
"""
import time
import numpy as np
import networkx as nx
from new_environment import GraphPartitionEnvironment
from agent_ppo import PPOAgent
from agent_ppo_gnn_simple import SimplePPOAgentGNN

def create_test_graphs():
    """åˆ›å»ºä¸åŒè§„æ¨¡çš„æµ‹è¯•å›¾"""
    graphs = {}
    
    # å°å›¾ï¼š10èŠ‚ç‚¹ (å½“å‰æµ‹è¯•)
    G_small = nx.random_geometric_graph(10, 0.5, seed=42)
    for i in range(10):
        G_small.nodes[i]['weight'] = np.random.randint(1, 10)
    graphs['small_10'] = G_small
    
    # ä¸­å›¾ï¼š20èŠ‚ç‚¹
    G_medium = nx.random_geometric_graph(20, 0.4, seed=42)
    for i in range(20):
        G_medium.nodes[i]['weight'] = np.random.randint(1, 10)
    graphs['medium_20'] = G_medium
    
    # å¤§å›¾ï¼š50èŠ‚ç‚¹
    G_large = nx.random_geometric_graph(50, 0.3, seed=42)
    for i in range(50):
        G_large.nodes[i]['weight'] = np.random.randint(1, 10)
    graphs['large_50'] = G_large
    
    return graphs

def quick_train_test(graph, agent_type, graph_name, episodes=100):
    """å¿«é€Ÿè®­ç»ƒæµ‹è¯•"""
    print(f"\nğŸ”„ æµ‹è¯• {graph_name} - {agent_type}")
    
    num_nodes = len(graph.nodes())
    num_partitions = 2
    max_steps = min(50, num_nodes * 2)  # è‡ªé€‚åº”æ­¥æ•°
    
    # åˆ›å»ºç¯å¢ƒ
    env = GraphPartitionEnvironment(graph, num_partitions, max_steps)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    if agent_type == "PPO":
        state_size = num_nodes * (num_partitions + 2)
        action_size = num_nodes * num_partitions
        agent = PPOAgent(state_size, action_size, {
            'learning_rate': 0.0003,
            'batch_size': min(32, max(8, num_nodes)),
            'ppo_epochs': 3,
            'use_tensorboard': False
        })
    else:  # SimplePPOGNN
        node_feature_dim = num_partitions + 2
        action_size = num_nodes * num_partitions
        agent = SimplePPOAgentGNN(node_feature_dim, action_size, {
            'learning_rate': 0.0001,
            'batch_size': min(32, max(8, num_nodes)),
            'ppo_epochs': 3,
            'hidden_dim': min(64, max(32, num_nodes)),
            'use_tensorboard': False
        })
    
    # å¿«é€Ÿè®­ç»ƒ
    best_reward = float('-inf')
    final_rewards = []
    start_time = time.time()
    
    for episode in range(episodes):
        if agent_type == "PPO":
            state, _ = env.reset()
        else:
            state, _ = env.reset(state_format='graph')
        
        total_reward = 0
        for step in range(max_steps):
            action = agent.act(state)
            if agent_type == "PPO":
                next_state, reward, done, _, _ = env.step(action)
            else:
                next_state, reward, done, _, _ = env.step(action)
                next_state = env.get_state('graph')
            
            agent.store_transition(reward, done)
            state = next_state
            total_reward += reward
            
            if done:
                break
        
        agent.update()
        final_rewards.append(total_reward)
        best_reward = max(best_reward, total_reward)
        
        # æ¯25ä¸ªepisodeæ‰“å°è¿›åº¦
        if (episode + 1) % 25 == 0:
            avg_reward = np.mean(final_rewards[-25:])
            print(f"  Episode {episode+1}: avg_reward={avg_reward:.1f}, best={best_reward:.1f}")
    
    training_time = time.time() - start_time
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    from metrics import evaluate_partition
    final_eval = evaluate_partition(graph, env.partition_assignment, num_partitions, print_results=False)
    
    return {
        'best_reward': best_reward,
        'avg_final_reward': np.mean(final_rewards[-10:]),  # æœ€å10ä¸ªepisodeå¹³å‡
        'training_time': training_time,
        'weight_variance': final_eval['weight_variance'],
        'edge_cut': final_eval['edge_cut'],
        'num_nodes': num_nodes,
        'num_edges': len(graph.edges())
    }

def run_scale_comparison():
    """è¿è¡Œå¤šè§„æ¨¡æ¯”è¾ƒ"""
    print("ğŸ¯ å¼€å§‹å¤šè§„æ¨¡å›¾æµ‹è¯•...")
    
    graphs = create_test_graphs()
    results = {}
    
    for graph_name, graph in graphs.items():
        print(f"\nğŸ“Š æµ‹è¯•å›¾: {graph_name} ({len(graph.nodes())}èŠ‚ç‚¹, {len(graph.edges())}è¾¹)")
        
        # æµ‹è¯•åŸç‰ˆPPO
        try:
            ppo_result = quick_train_test(graph, "PPO", graph_name, episodes=100)
            results[f"{graph_name}_PPO"] = ppo_result
        except Exception as e:
            print(f"  âŒ PPOæµ‹è¯•å¤±è´¥: {e}")
            results[f"{graph_name}_PPO"] = None
        
        # æµ‹è¯•ç®€åŒ–GNN-PPO
        try:
            gnn_result = quick_train_test(graph, "SimplePPOGNN", graph_name, episodes=100)
            results[f"{graph_name}_SimplePPOGNN"] = gnn_result
        except Exception as e:
            print(f"  âŒ SimplePPOGNNæµ‹è¯•å¤±è´¥: {e}")
            results[f"{graph_name}_SimplePPOGNN"] = None
    
    # åˆ†æç»“æœ
    print("\n" + "="*80)
    print("ğŸ“ˆ å¤šè§„æ¨¡æµ‹è¯•ç»“æœåˆ†æ")
    print("="*80)
    
    for graph_name in ['small_10', 'medium_20', 'large_50']:
        ppo_key = f"{graph_name}_PPO"
        gnn_key = f"{graph_name}_SimplePPOGNN"
        
        if results.get(ppo_key) and results.get(gnn_key):
            ppo_res = results[ppo_key]
            gnn_res = results[gnn_key]
            
            print(f"\nğŸ” {graph_name.upper()} ({ppo_res['num_nodes']}èŠ‚ç‚¹):")
            print(f"  æƒé‡æ–¹å·®:    PPO={ppo_res['weight_variance']:.1f}  vs  GNN={gnn_res['weight_variance']:.1f}")
            print(f"  è¾¹åˆ‡å‰²:      PPO={ppo_res['edge_cut']:.1f}      vs  GNN={gnn_res['edge_cut']:.1f}")
            print(f"  è®­ç»ƒæ—¶é—´:    PPO={ppo_res['training_time']:.1f}s   vs  GNN={gnn_res['training_time']:.1f}s")
            print(f"  æœ€ç»ˆå¥–åŠ±:    PPO={ppo_res['avg_final_reward']:.1f}    vs  GNN={gnn_res['avg_final_reward']:.1f}")
            
            # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
            if ppo_res['weight_variance'] > 0:
                var_improvement = (ppo_res['weight_variance'] - gnn_res['weight_variance']) / ppo_res['weight_variance'] * 100
                print(f"  æ–¹å·®æ”¹è¿›:    {var_improvement:+.1f}% (è´Ÿæ•°=GNNæ›´å·®)")
            
            if ppo_res['avg_final_reward'] != 0:
                reward_improvement = (gnn_res['avg_final_reward'] - ppo_res['avg_final_reward']) / abs(ppo_res['avg_final_reward']) * 100
                print(f"  å¥–åŠ±æ”¹è¿›:    {reward_improvement:+.1f}% (æ­£æ•°=GNNæ›´å¥½)")
    
    return results

if __name__ == "__main__":
    results = run_scale_comparison()
    print("\nğŸ å¤šè§„æ¨¡æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ åˆ†æå»ºè®®:")
    print("  - å¦‚æœå°å›¾ä¸ŠGNNåŠ£åŠ¿æ˜æ˜¾ï¼Œå±æ­£å¸¸ç°è±¡")
    print("  - å¦‚æœä¸­å›¾/å¤§å›¾ä¸ŠGNNä»æ— ä¼˜åŠ¿ï¼Œéœ€é‡æ–°è€ƒè™‘æ¶æ„")
    print("  - å…³æ³¨è®­ç»ƒæ—¶é—´vsæ€§èƒ½çš„æƒè¡¡") 