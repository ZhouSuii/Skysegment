import optuna
import os
import json
import networkx as nx
import numpy as np

# å¯¼å…¥æ‚¨é¡¹ç›®ä¸­çš„æ ¸å¿ƒå‡½æ•°
# æ³¨æ„ï¼šæˆ‘ä»¬å°†ç›´æ¥åœ¨objectiveå‡½æ•°ä¸­å¤ç”¨è®­ç»ƒé€»è¾‘ï¼Œè€Œä¸æ˜¯ç›´æ¥è°ƒç”¨train_gnn_ppo_agent
from run_experiments import load_graph_from_file
from new_environment import GraphPartitionEnvironment
from agent_ppo_gnn_simple import SimplePPOAgentGNN
from metrics import evaluate_partition

def objective(trial):
    """
    è¿™æ˜¯Optunaçš„ç›®æ ‡å‡½æ•°ï¼ŒåŒ…å«äº†å®Œæ•´çš„è®­ç»ƒå’Œå‰ªæé€»è¾‘ã€‚
    """
    print(f"\n===== Trial #{trial.number} starting... =====")

    # 1. å®šä¹‰è¶…å‚æ•°çš„æœç´¢ç©ºé—´
    config = {
        "episodes": 500,
        "max_steps": 100,
        "gnn_ppo_config": {
            "potential_weights": {
                "variance": trial.suggest_float("variance_weight", 0.1, 20.0, log=True),
                "edge_cut": trial.suggest_float("edge_cut_weight", 0.1, 10.0, log=True),
                "modularity": trial.suggest_float("modularity_weight", 0.1, 10.0, log=True),
            },
            "entropy_coef": trial.suggest_float("entropy_coef", 1e-3, 0.1, log=True),
            "clip_ratio": trial.suggest_float("clip_ratio", 0.1, 0.3),
            "learning_rate": trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True),
            "hidden_dim": 2048,
            "num_partitions": num_partitions
        }
    }

    # 2. åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“ (è¿™éƒ¨åˆ†é€»è¾‘æ¥è‡ª train_gnn_ppo_agent)
    gnn_ppo_config = config["gnn_ppo_config"]
    env = GraphPartitionEnvironment(
        graph,
        num_partitions,
        config["max_steps"],
        gamma=gnn_ppo_config.get('gamma', 0.99),
        potential_weights=gnn_ppo_config["potential_weights"]
    )
    node_feature_dim = num_partitions + 2
    action_size = len(graph.nodes()) * num_partitions
    agent = SimplePPOAgentGNN(node_feature_dim, action_size, gnn_ppo_config)

    # 3. è®­ç»ƒå¾ªç¯ï¼Œå¹¶åœ¨å…¶ä¸­åŠ å…¥å‰ªæé€»è¾‘
    for e in range(config["episodes"]):
        graph_state, _ = env.reset(state_format='graph')
        
        for step in range(config["max_steps"]):
            action = agent.act(graph_state)
            _, reward, done, _, _ = env.step(action)
            next_graph_state = env.get_state('graph')
            agent.store_transition(reward, done)
            graph_state = next_graph_state
            if done:
                break
        
        agent.update()

        # === å‰ªææ ¸å¿ƒé€»è¾‘ ===
        # a. åœ¨è®­ç»ƒä¸­é€”è®¡ç®—ä¸€ä¸ªä¸´æ—¶è¯„ä¼°åˆ†æ•°
        eval_results = evaluate_partition(graph, env.partition_assignment, num_partitions, print_results=False)
        intermediate_score = (1.0 * eval_results["weight_variance"]) + (10000 * eval_results["normalized_cut"])
        
        # b. å‘Optunaæ±‡æŠ¥å½“å‰çš„åˆ†æ•°å’Œæ­¥æ•°
        trial.report(intermediate_score, e)

        # c. è¯¢é—®Optunaæ˜¯å¦åº”è¯¥å‰ªæ
        if trial.should_prune():
            print(f"Trial #{trial.number} pruned at episode {e} with score {intermediate_score:.2f}.")
            raise optuna.exceptions.TrialPruned()

    # 4. å¦‚æœè®­ç»ƒæ­£å¸¸å®Œæˆï¼Œè¿”å›æœ€ç»ˆåˆ†æ•°
    final_eval_results = evaluate_partition(graph, env.partition_assignment, num_partitions, print_results=False)
    final_score = (1.0 * final_eval_results["weight_variance"]) + (10000 * final_eval_results["normalized_cut"])
    
    if final_eval_results["weight_imbalance"] > 2.0:
        final_score += 1e9

    print(f"TRIAL #{trial.number} finished.")
    print(f"  - Params: {trial.params}")
    print(f"  - Results: variance={final_eval_results['weight_variance']:.2f}, norm_cut={final_eval_results['normalized_cut']:.4f}")
    print(f"  - FINAL SCORE: {final_score:.2f} (the lower the better)")

    return final_score


if __name__ == "__main__":
    # åŠ è½½å›¾
    real_graph_path = "ctu_airspace_graph_1900_2000_kmeans.graphml"
    print(f"ğŸ”„ Loading graph for tuning: {real_graph_path}")
    graph = load_graph_from_file(real_graph_path)
    # é‡æ–°ç¼–å·
    node_mapping = {old_node: i for i, old_node in enumerate(graph.nodes())}
    graph = nx.relabel_nodes(graph, node_mapping)
    num_partitions = 3 if graph.number_of_nodes() > 15 else 2

    # åˆ›å»ºä¸€ä¸ªOptuna "study" å¯¹è±¡
    study_name = "gnn_ppo_tuning_study"
    storage_name = f"sqlite:///{study_name}.db"
    
    print(f"ğŸš€ Starting Optuna study: {study_name}")
    print(f"Results will be stored in: {storage_name}")
    
    # === æ–°å¢ï¼šé…ç½®å‰ªæå™¨ ===
    # æˆ‘ä»¬ä½¿ç”¨ä¸­ä½æ•°å‰ªæå™¨ï¼Œå®ƒä¼šåœ¨è‹¥å¹²æ­¥åï¼Œæ¯”è¾ƒå½“å‰è¯•éªŒå’Œå†å²è¯•éªŒçš„ä¸­ä½æ•°è¡¨ç°
    pruner = optuna.pruners.MedianPruner(
        n_startup_trials=5,  # å‰5æ¬¡è¯•éªŒä¸åšå‰ªæï¼Œç”¨äºæ”¶é›†åŸºå‡†æ•°æ®
        n_warmup_steps=100,   # å‰50ä¸ªepisodesä¸åšå‰ªæï¼Œè®©æ¨¡å‹å…ˆçƒ­èº«
        interval_steps=10    # æ¯10ä¸ªepisodesæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦è¦å‰ªæ
    )
    
    study = optuna.create_study(
        study_name=study_name,
        storage=storage_name,
        direction="minimize", 
        load_if_exists=True,
        pruner=pruner  # <-- å°†å‰ªæå™¨åº”ç”¨åˆ°studyä¸­
    )

    # å¯åŠ¨ä¼˜åŒ–
    study.optimize(objective, n_trials=50)

    # å®éªŒç»“æŸï¼Œæ‰“å°æœ€ä½³ç»“æœ
    print("\n\nğŸ‰ğŸ‰ğŸ‰ TUNING COMPLETE! ğŸ‰ğŸ‰ğŸ‰")
    print(f"Best trial: #{study.best_trial.number}")
    print(f"  - Score: {study.best_trial.value:.2f}")
    print("  - Best hyperparameters:")
    for key, value in study.best_trial.params.items():
        print(f"    - {key}: {value}")

    # å°†æœ€ä½³å‚æ•°ä¿å­˜åˆ°JSONæ–‡ä»¶
    best_params_config = {
        "gnn_ppo_config": {
            "potential_weights": {
                "variance": study.best_trial.params["variance_weight"],
                "edge_cut": study.best_trial.params["edge_cut_weight"],
                "modularity": study.best_trial.params["modularity_weight"],
            },
            "learning_rate": study.best_trial.params.get("learning_rate"), # .get()æ›´å®‰å…¨
            "entropy_coef": study.best_trial.params["entropy_coef"],
            "clip_ratio": study.best_trial.params["clip_ratio"],
        }
    }
    
    output_path = "configs/best_params_generated.json"
    with open(output_path, "w") as f:
        json.dump(best_params_config, f, indent=4)
        
    print(f"\nâœ… Best parameters saved to {output_path}")
    print("You can now use this file to configure your final experiment run.") 