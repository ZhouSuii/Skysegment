import optuna
import os
import json
import networkx as nx
import numpy as np

# å¯¼å…¥æ‚¨é¡¹ç›®ä¸­çš„æ ¸å¿ƒå‡½æ•°
# æ³¨æ„ï¼šæˆ‘ä»¬å°†ç›´æ¥åœ¨objectiveå‡½æ•°ä¸­å¤ç”¨è®­ç»ƒé€»è¾‘ï¼Œè€Œä¸æ˜¯ç›´æ¥è°ƒç”¨train_gnn_ppo_agent
from run_experiments import load_graph_from_file
from new_environment import GraphPartitionEnvironment
from agent_ppo_gnn_simple import SimplePPOAgentGNN
from metrics import evaluate_partition

# --- æ–°å¢è¾…åŠ©å‡½æ•° ---
def calculate_max_variance(graph, num_partitions):
    """
    è®¡ç®—ç»™å®šå›¾çš„ç†è®ºæœ€å¤§æƒé‡æ–¹å·®ã€‚
    è¿™ç§æƒ…å†µå‘ç”Ÿåœ¨æ‰€æœ‰èŠ‚ç‚¹æƒé‡éƒ½é›†ä¸­åœ¨ä¸€ä¸ªåˆ†åŒºï¼Œè€Œå…¶ä»–åˆ†åŒºä¸ºç©ºæ—¶ã€‚
    è¿™ä¸ªå€¼å°†ä½œä¸ºæˆ‘ä»¬åç»­å½’ä¸€åŒ–çš„åŸºå‡†ã€‚
    """
    if not graph.nodes or num_partitions <= 1:
        return 1.0 # è¿”å›1.0ä»¥é¿å…é™¤ä»¥é›¶
    
    # å›¾ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„æƒé‡æ€»å’Œ
    total_weight = sum(d.get('weight', 1.0) for _, d in graph.nodes(data=True))
    
    # æ„é€ æœ€åæƒ…å†µä¸‹çš„åˆ†åŒºæƒé‡åˆ—è¡¨ï¼ˆæ‰€æœ‰æƒé‡åœ¨ä¸€ä¸ªåˆ†åŒºï¼‰
    worst_case_weights = [total_weight] + [0.0] * (num_partitions - 1)
    
    # è®¡ç®—è¿™ç§æœ€ååˆ†å¸ƒä¸‹çš„æ–¹å·®
    max_var = np.var(worst_case_weights)
    
    return max_var if max_var > 0 else 1.0

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨åªè®¡ç®—ä¸€æ¬¡çš„æœ€å¤§æ–¹å·®ï¼Œä»¥ä¾¿objectiveå‡½æ•°å¯ä»¥è®¿é—®
# This is a practical approach for Optuna's study.optimize interface.
max_variance = 1.0

def objective(trial):
    """
    è¿™æ˜¯Optunaçš„ç›®æ ‡å‡½æ•°ï¼ŒåŒ…å«äº†å®Œæ•´çš„è®­ç»ƒå’Œå‰ªæé€»è¾‘ã€‚
    """
    print(f"\n===== Trial #{trial.number} starting... =====")

    # 1. å®šä¹‰è¶…å‚æ•°çš„æœç´¢ç©ºé—´
    config = {
        "episodes": 500,
        "max_steps": 100,
        "gnn_ppo_config": {
            "potential_weights": {
                "variance": trial.suggest_float("variance_weight", 0.1, 20.0, log=True),
                "edge_cut": trial.suggest_float("edge_cut_weight", 0.1, 10.0, log=True),
                "modularity": trial.suggest_float("modularity_weight", 0.1, 10.0, log=True),
            },
            # === å¼ºçƒˆå»ºè®®æ–°å¢çš„æœç´¢å‚æ•° ===
            "gamma": trial.suggest_float("gamma", 0.9, 0.999),
            "gae_lambda": trial.suggest_float("gae_lambda", 0.9, 0.99),
            "hidden_dim": trial.suggest_categorical("hidden_dim", [256, 512, 1024, 2048]),
            "entropy_coef": trial.suggest_float("entropy_coef", 1e-3, 0.1, log=True),
            "clip_ratio": trial.suggest_float("clip_ratio", 0.1, 0.3),
            "learning_rate": 0.00005,
            
            # === æ–°å¢ï¼šä¸ default.json å¯¹é½çš„å›ºå®šå‚æ•° ===
            "ppo_epochs": 6,
            "batch_size": 512,
            "value_coef": 0.43197785729901544,
            "update_frequency": 2048,
            
            "num_partitions": num_partitions
        }
    }

    # 2. åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“ (è¿™éƒ¨åˆ†é€»è¾‘æ¥è‡ª train_gnn_ppo_agent)
    gnn_ppo_config = config["gnn_ppo_config"]
    env = GraphPartitionEnvironment(
        graph,
        num_partitions,
        config["max_steps"],
        gamma=gnn_ppo_config.get('gamma', 0.99),
        potential_weights=gnn_ppo_config["potential_weights"]
    )
    node_feature_dim = num_partitions + 2
    action_size = len(graph.nodes()) * num_partitions
    agent = SimplePPOAgentGNN(node_feature_dim, action_size, gnn_ppo_config)

    # 3. è®­ç»ƒå¾ªç¯ï¼Œå¹¶åœ¨å…¶ä¸­åŠ å…¥å‰ªæé€»è¾‘
    for e in range(config["episodes"]):
        graph_state, _ = env.reset(state_format='graph')
        
        for step in range(config["max_steps"]):
            action = agent.act(graph_state)
            _, reward, done, _, _ = env.step(action)
            next_graph_state = env.get_state('graph')
            agent.store_transition(reward, done)
            graph_state = next_graph_state
            if done:
                break
        
        agent.update()

        # === å‰ªææ ¸å¿ƒé€»è¾‘ ===
        # a. åœ¨è®­ç»ƒä¸­é€”è®¡ç®—ä¸€ä¸ªä¸´æ—¶è¯„ä¼°åˆ†æ•°
        eval_results = evaluate_partition(graph, env.partition_assignment, num_partitions, print_results=False)
        
        # === æ–°çš„ã€æ›´ç§‘å­¦çš„è¯„åˆ†é€»è¾‘ ===
        # 1. å°†ä¸åŒé‡çº²çš„æŒ‡æ ‡å½’ä¸€åŒ–åˆ°åŒä¸€ä¸ª [0, 1] åŒºé—´
        normalized_variance = eval_results["weight_variance"] / max_variance
        normalized_ncut = eval_results["normalized_cut"] / 2.0  # N-cutçš„ç†è®ºä¸Šç•Œæ˜¯2.0

        # 2. ä½¿ç”¨åŠ æƒå’Œè®¡ç®—æœ€ç»ˆåˆ†æ•°ã€‚alphaå‚æ•°ä»£è¡¨äº†æˆ‘ä»¬å¯¹ä¸¤ä¸ªç›®æ ‡çš„åå¥½ã€‚
        alpha = 0.5  # alpha=0.5 ä»£è¡¨æˆ‘ä»¬è®¤ä¸ºæ–¹å·®å’Œåˆ‡è¾¹åŒç­‰é‡è¦
        intermediate_score = alpha * normalized_variance + (1 - alpha) * normalized_ncut

        # b. å‘Optunaæ±‡æŠ¥å½“å‰çš„åˆ†æ•°å’Œæ­¥æ•°
        trial.report(intermediate_score, e)

        # c. è¯¢é—®Optunaæ˜¯å¦åº”è¯¥å‰ªæ
        if trial.should_prune():
            print(f"Trial #{trial.number} pruned at episode {e} with score {intermediate_score:.4f}.")
            raise optuna.exceptions.TrialPruned()

    # 4. å¦‚æœè®­ç»ƒæ­£å¸¸å®Œæˆï¼Œè¿”å›æœ€ç»ˆåˆ†æ•°
    final_eval_results = evaluate_partition(graph, env.partition_assignment, num_partitions, print_results=False)
    
    # é‡å¤ä½¿ç”¨ç›¸åŒçš„ç§‘å­¦è¯„åˆ†é€»è¾‘
    normalized_variance = final_eval_results["weight_variance"] / max_variance
    normalized_ncut = final_eval_results["normalized_cut"] / 2.0
    alpha = 0.5
    final_score = alpha * normalized_variance + (1 - alpha) * normalized_ncut
    
    if final_eval_results["weight_imbalance"] > 2.0:
        final_score += 1e9 # ä¿æŒå¯¹ä¸¥é‡ä¸å¹³è¡¡çš„å·¨å¤§æƒ©ç½š

    print(f"TRIAL #{trial.number} finished.")
    print(f"  - Params: {trial.params}")
    print(f"  - Raw Results: variance={final_eval_results['weight_variance']:.2f}, norm_cut={final_eval_results['normalized_cut']:.4f}")
    print(f"  - Normalized Score Components: norm_var={normalized_variance:.4f}, norm_ncut={normalized_ncut:.4f}")
    print(f"  - FINAL SCORE: {final_score:.4f} (the lower the better, based on normalized metrics)")

    return final_score


if __name__ == "__main__":
    # åŠ è½½å›¾
    real_graph_path = "ctu_airspace_graph_1900_2000_kmeans.graphml"
    print(f"ğŸ”„ Loading graph for tuning: {real_graph_path}")
    graph = load_graph_from_file(real_graph_path)
    # é‡æ–°ç¼–å·
    node_mapping = {old_node: i for i, old_node in enumerate(graph.nodes())}
    graph = nx.relabel_nodes(graph, node_mapping)
    num_partitions = 3 if graph.number_of_nodes() > 15 else 2

    # --- æ–°å¢: åœ¨å¼€å§‹å‰ï¼Œè®¡ç®—ä¸€æ¬¡ç†è®ºæœ€å¤§æ–¹å·® ---
    max_variance = calculate_max_variance(graph, num_partitions)
    print(f"âš–ï¸  Calculated theoretical max variance for normalization: {max_variance:.2f}")

    # åˆ›å»ºä¸€ä¸ªOptuna "study" å¯¹è±¡
    study_name = "gnn_ppo_tuning_study"
    storage_name = f"sqlite:///{study_name}.db"
    
    print(f"ğŸš€ Starting Optuna study: {study_name}")
    print(f"Results will be stored in: {storage_name}")
    
    # === æ–°å¢ï¼šé…ç½®å‰ªæå™¨ ===
    # æˆ‘ä»¬ä½¿ç”¨ä¸­ä½æ•°å‰ªæå™¨ï¼Œå®ƒä¼šåœ¨è‹¥å¹²æ­¥åï¼Œæ¯”è¾ƒå½“å‰è¯•éªŒå’Œå†å²è¯•éªŒçš„ä¸­ä½æ•°è¡¨ç°
    pruner = optuna.pruners.MedianPruner(
        n_startup_trials=5,  # å‰5æ¬¡è¯•éªŒä¸åšå‰ªæï¼Œç”¨äºæ”¶é›†åŸºå‡†æ•°æ®
        n_warmup_steps=100,   # å‰50ä¸ªepisodesä¸åšå‰ªæï¼Œè®©æ¨¡å‹å…ˆçƒ­èº«
        interval_steps=10    # æ¯10ä¸ªepisodesæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦è¦å‰ªæ
    )
    
    study = optuna.create_study(
        study_name=study_name,
        storage=storage_name,
        direction="minimize", 
        load_if_exists=True,
        pruner=pruner  # <-- å°†å‰ªæå™¨åº”ç”¨åˆ°studyä¸­
    )

    # å¯åŠ¨ä¼˜åŒ–
    study.optimize(objective, n_trials=50, show_progress_bar=True)

    # å®éªŒç»“æŸï¼Œæ‰“å°æœ€ä½³ç»“æœ
    print("\n\nğŸ‰ğŸ‰ğŸ‰ TUNING COMPLETE! ğŸ‰ğŸ‰ğŸ‰")
    print(f"Best trial: #{study.best_trial.number}")
    print(f"  - Score: {study.best_trial.value:.2f}")
    print("  - Best hyperparameters:")
    # === ä¿®å¤ï¼šåˆå¹¶å›ºå®šå‚æ•°å’Œæœç´¢åˆ°çš„å‚æ•° ===
    final_params = study.best_trial.params.copy()
    # æ‰‹åŠ¨åŠ å…¥åœ¨æœç´¢ä¸­è¢«å›ºå®šçš„å‚æ•°
    final_params["learning_rate"] = 0.00005
    final_params["ppo_epochs"] = 6
    final_params["batch_size"] = 512
    final_params["value_coef"] = 0.43197785729901544
    final_params["update_frequency"] = 2048

    for key, value in final_params.items():
        print(f"    - {key}: {value}")

    # === ä¿®å¤ï¼šå°†æ‰€æœ‰ç›¸å…³çš„æœ€ä½³å‚æ•°ä¿å­˜åˆ°JSONæ–‡ä»¶ ===
    best_params_config = {
        "gnn_ppo_config": {
            "potential_weights": {
                "variance": final_params["variance_weight"],
                "edge_cut": final_params["edge_cut_weight"],
                "modularity": final_params["modularity_weight"],
            },
            "learning_rate": final_params["learning_rate"],
            "gamma": final_params["gamma"],
            "gae_lambda": final_params["gae_lambda"],
            "hidden_dim": final_params["hidden_dim"],
            "entropy_coef": final_params["entropy_coef"],
            "clip_ratio": final_params["clip_ratio"],
            "ppo_epochs": final_params["ppo_epochs"],
            "batch_size": final_params["batch_size"],
            "value_coef": final_params["value_coef"],
            "update_frequency": final_params["update_frequency"]
        }
    }
    
    output_path = "configs/best_params_generated.json"
    with open(output_path, "w") as f:
        json.dump(best_params_config, f, indent=4)
        
    print(f"\nâœ… Best parameters saved to {output_path}")
    print("You can now use this file to configure your final experiment run.") 