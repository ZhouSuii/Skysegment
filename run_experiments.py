# run experiments
import os
import time
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
import json
import torch
from tqdm import tqdm
import random

# é…ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
try:
    # å°è¯•ä½¿ç”¨æ–‡æ³‰é©¿å¾®ç±³é»‘å­—ä½“
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'SimHei', 'DejaVu Sans', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
    
    # éªŒè¯å­—ä½“åŠ è½½
    plt.rcParams['font.family'] = 'sans-serif'  # ä½¿ç”¨æ— è¡¬çº¿å­—ä½“
except Exception as e:
    print(f"å­—ä½“é…ç½®é”™è¯¯: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ä½“")

from new_environment import GraphPartitionEnvironment
from agent_dqn_basic import DQNAgent
from metrics import evaluate_partition
from baselines import random_partition, weighted_greedy_partition, metis_partition, spectral_partition
from agent_gnn import GNNDQNAgent
from agent_ppo import PPOAgent
from agent_ppo_gnn_simple import SimplePPOAgentGNN
from metrics import calculate_weight_variance, calculate_partition_weights

def set_seed(seed=42):
    """
    è®¾ç½®æ‰€æœ‰éšæœºç§å­ï¼Œç¡®ä¿æ¯ä¸ªç®—æ³•éƒ½ä»ç›¸åŒçš„éšæœºçŠ¶æ€å¼€å§‹
    è¿™æ˜¯è§£å†³ç®—æ³•é—´å…¬å¹³å¯¹æ¯”çš„å…³é”®å‡½æ•°
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¡®ä¿CUDAæ“ä½œçš„ç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ç¡®ä¿Python hashçš„ç¡®å®šæ€§
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    print(f"ğŸ”§ é‡ç½®éšæœºç§å­ä¸º: {seed}")

def create_test_graph(num_nodes=10, seed=42):
    """åˆ›å»ºæµ‹è¯•å›¾ï¼Œå¸¦æœ‰èŠ‚ç‚¹æƒé‡"""
    np.random.seed(seed)
    G = nx.random_geometric_graph(num_nodes, 0.5, seed=seed)

    # æ·»åŠ èŠ‚ç‚¹æƒé‡
    for i in range(num_nodes):
        G.nodes[i]['weight'] = np.random.randint(1, 10)

    return G


def load_graph_from_file(filepath):
    """ä»æ–‡ä»¶åŠ è½½å›¾"""
    try:
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½æ–¹æ³•
        extension = os.path.splitext(filepath)[1]
        if extension == '.graphml':
            G = nx.read_graphml(filepath)
        elif extension == '.gexf':
            G = nx.read_gexf(filepath)
        elif extension == '.edgelist':
            G = nx.read_edgelist(filepath)
        else:
            # é»˜è®¤å°è¯•pickleåŠ è½½
            G = nx.read_gpickle(filepath)

        # å¦‚æœæ²¡æœ‰èŠ‚ç‚¹æƒé‡ï¼Œæ·»åŠ é»˜è®¤æƒé‡1
        for node in G.nodes():
            if 'weight' not in G.nodes[node]:
                G.nodes[node]['weight'] = 1

        return G
    except Exception as e:
        print(f"åŠ è½½å›¾æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None


def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return {}


def train_dqn_agent(graph, num_partitions, config, results_dir="results"):
    # æ¯ä¸ªç®—æ³•éƒ½ä»ç›¸åŒçš„éšæœºçŠ¶æ€å¼€å§‹
    set_seed(42)
    """è®­ç»ƒDQNæ™ºèƒ½ä½“"""
    # è·å–é…ç½®å‚æ•°
    episodes = config.get("episodes", 1000)
    max_steps = config.get("max_steps", 100)
    batch_size = config.get("dqn_config", {}).get("batch_size", config.get("batch_size", 32))
    dqn_config = config.get("dqn_config", {}) # è·å–DQNé…ç½®

    # --- ä¿®æ”¹ï¼šä½¿ç”¨ new_environment å¹¶ä¼ é€’å‚æ•° ---
    # ä½¿ç”¨é»˜è®¤çš„ potential_weights æˆ–ä»é…ç½®åŠ è½½
    default_potential_weights = {'variance': 1.0, 'edge_cut': 1.0, 'modularity': 1.0}
    potential_weights = config.get("potential_weights", default_potential_weights)
    env = GraphPartitionEnvironment(
        graph,
        num_partitions,
        max_steps,
        gamma=dqn_config.get('gamma', 0.95), # ä»DQNé…ç½®è·å–gamma
        potential_weights=potential_weights
    )

    # åˆå§‹åŒ–DQNä»£ç†
    num_nodes = len(graph.nodes())
    # --- ä¿®æ”¹ï¼šæ›´æ–°çŠ¶æ€å¤§å°è®¡ç®— (+1 for node weights) ---
    state_size = num_nodes * (num_partitions + 2)
    # --- ä¿®æ”¹ç»“æŸ ---
    action_size = num_nodes * num_partitions
    dqn_config['batch_size'] = batch_size # ä¼ é€’æ­£ç¡®çš„batch_sizeç»™agent
    agent = DQNAgent(state_size, action_size, dqn_config)

    best_reward = float('-inf')
    best_partition = None
    rewards_history = []
    loss_history = []  # æ–°å¢ï¼šè®°å½•æŸå¤±å†å²
    variance_history = []  # æ–°å¢ï¼šè®°å½•æ–¹å·®å†å²
    
    # æ–°å¢: æŒ‰ episode è®°å½•çš„åˆ—è¡¨
    episode_rewards = []
    episode_variances = []

    # è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(range(episodes), desc="è®­ç»ƒDQN")
    for e in progress_bar:
        state, _ = env.reset()
        total_reward = 0
        done = False

        for step in range(max_steps):
            action = agent.act(state)
            next_state, reward, done, _, info = env.step(action)

            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

            if done:
                break
        
        # ä½¿ç”¨ agent.memory_counter è·å–å®é™…å­˜å‚¨çš„æ ·æœ¬æ•°
        memory_size = min(agent.memory_counter, agent.memory_capacity)
        
        # è®¡ç®—å½“å‰ episode çš„æ–¹å·®
        current_variance = calculate_weight_variance(graph, env.partition_assignment, num_partitions)

        # è¿›è¡Œç»éªŒå›æ”¾å­¦ä¹ 
        if memory_size >= batch_size:
            loss = agent.replay(batch_size)  # è·å–è¿”å›çš„æŸå¤±å€¼
            
            # ä»…åœ¨æœ‰æ•ˆæ›´æ–°æ—¶è®°å½•æ‰€æœ‰æŒ‡æ ‡
            loss_history.append(loss)
            rewards_history.append(total_reward)
            variance_history.append(current_variance)

        # æŒ‰ episode è®°å½•æ ¸å¿ƒæŒ‡æ ‡
        episode_rewards.append(total_reward)
        episode_variances.append(current_variance)

        # æ›´æ–°æœ€ä½³åˆ’åˆ†
        if total_reward > best_reward:
            best_reward = total_reward
            best_partition = env.partition_assignment.copy()

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'reward': total_reward,
            'best': best_reward,
            'epsilon': agent.epsilon,
            'loss': loss_history[-1] if loss_history else 0,
            'variance': variance_history[-1] if variance_history else 0
        })

        # æ¯50ä¸ªepisodeä¿å­˜ä¸€æ¬¡æ¨¡å‹
        if (e + 1) % 50 == 0:
            os.makedirs(f"{results_dir}/models", exist_ok=True)
            agent.save_model(f"{results_dir}/models/dqn_model_{len(graph.nodes())}nodes_{num_partitions}parts_temp.pt")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    os.makedirs(f"{results_dir}/models", exist_ok=True)
    agent.save_model(f"{results_dir}/models/dqn_model_{len(graph.nodes())}nodes_{num_partitions}parts.pt")

    return best_partition, rewards_history, loss_history, variance_history, episode_rewards, episode_variances


def train_gnn_agent(graph, num_partitions, config, results_dir="results"):
    # æ¯ä¸ªç®—æ³•éƒ½ä»ç›¸åŒçš„éšæœºçŠ¶æ€å¼€å§‹
    set_seed(42)
    """è®­ç»ƒGNNæ™ºèƒ½ä½“"""
    # è·å–é…ç½®å‚æ•°
    episodes = config.get("episodes", 500)
    max_steps = config.get("max_steps", 100)
    batch_size = config.get("batch_size", 32)
    gnn_config = config.get("gnn_config", {}) # è·å–GNNé…ç½®

    # --- ä¿®æ”¹ï¼šä½¿ç”¨ new_environment å¹¶ä¼ é€’å‚æ•° ---
    default_potential_weights = {'variance': 1.0, 'edge_cut': 1.0, 'modularity': 1.0}
    potential_weights = config.get("potential_weights", default_potential_weights)
    env = GraphPartitionEnvironment(
        graph,
        num_partitions,
        max_steps,
        gamma=gnn_config.get('gamma', 0.95), # ä»GNNé…ç½®è·å–gamma
        potential_weights=potential_weights
    )
    # --- ä¿®æ”¹ç»“æŸ ---

    # åˆå§‹åŒ–GNN-DQNä»£ç†
    agent = GNNDQNAgent(graph, num_partitions, gnn_config)

    best_reward = float('-inf')
    best_partition = None
    rewards_history = []
    loss_history = []  # æ–°å¢ï¼šè®°å½•æŸå¤±å†å²
    variance_history = []  # æ–°å¢ï¼šè®°å½•æ–¹å·®å†å²
    
    # æ–°å¢: æŒ‰ episode è®°å½•çš„åˆ—è¡¨
    episode_rewards = []
    episode_variances = []

    # è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(range(episodes), desc="è®­ç»ƒGNN-DQN")
    for e in progress_bar:
        state, _ = env.reset()
        total_reward = 0
        done = False

        for step in range(max_steps):
            action = agent.act(state)
            next_state, reward, done, _, info = env.step(action)

            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward
            
            if done:
                break
        
        # è®¡ç®—å½“å‰ episode çš„æ–¹å·®
        current_variance = calculate_weight_variance(graph, env.partition_assignment, num_partitions)

        # è¿›è¡Œç»éªŒå›æ”¾å­¦ä¹ 
        memory_size = min(agent.memory_counter, agent.memory_capacity)
        if memory_size >= batch_size:
            loss = agent.replay() # è·å–è¿”å›çš„æŸå¤±å€¼
            
            # ä»…åœ¨æœ‰æ•ˆæ›´æ–°æ—¶è®°å½•æ‰€æœ‰æŒ‡æ ‡
            loss_history.append(loss)
            rewards_history.append(total_reward)
            variance_history.append(current_variance)

        # æŒ‰ episode è®°å½•æ ¸å¿ƒæŒ‡æ ‡
        episode_rewards.append(total_reward)
        episode_variances.append(current_variance)

        # æ›´æ–°æœ€ä½³åˆ’åˆ†
        if total_reward > best_reward:
            best_reward = total_reward
            best_partition = env.partition_assignment.copy()

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'reward': total_reward,
            'best': best_reward,
            'epsilon': agent.epsilon,
            'loss': loss_history[-1] if loss_history else 0,
            'variance': variance_history[-1] if variance_history else 0
        })

    # ä¿å­˜æ¨¡å‹
    os.makedirs(f"{results_dir}/models", exist_ok=True)
    agent.save_model(f"{results_dir}/models/gnn_dqn_model_{len(graph.nodes())}nodes_{num_partitions}parts.pt")
    
    return best_partition, rewards_history, loss_history, variance_history, episode_rewards, episode_variances


def train_ppo_agent(graph, num_partitions, config, results_dir="results"):
    # æ¯ä¸ªç®—æ³•éƒ½ä»ç›¸åŒçš„éšæœºçŠ¶æ€å¼€å§‹
    set_seed(42)
    """è®­ç»ƒPPOæ™ºèƒ½ä½“"""
    # è·å–é…ç½®å‚æ•°
    episodes = config.get("episodes", 500)
    max_steps = config.get("max_steps", 100)
    ppo_config = config.get("ppo_config", {}) # è·å–PPOé…ç½®

    # --- ä¿®æ”¹ï¼šä½¿ç”¨ new_environment å¹¶ä¼ é€’å‚æ•° ---
    default_potential_weights = {'variance': 1.0, 'edge_cut': 1.0, 'modularity': 1.0}
    potential_weights = config.get("potential_weights", default_potential_weights)
    env = GraphPartitionEnvironment(
        graph,
        num_partitions,
        max_steps,
        gamma=ppo_config.get('gamma', 0.99), # ä»PPOé…ç½®è·å–gamma
        potential_weights=potential_weights
    )
    # --- ä¿®æ”¹ç»“æŸ ---

    # åˆå§‹åŒ–PPOä»£ç†
    num_nodes = len(graph.nodes())
    # --- ä¿®æ”¹ï¼šæ›´æ–°çŠ¶æ€å¤§å°è®¡ç®— (+1 for node weights) ---
    state_size = num_nodes * (num_partitions + 2)
    # --- ä¿®æ”¹ç»“æŸ ---
    action_size = num_nodes * num_partitions
    agent = PPOAgent(state_size, action_size, ppo_config)

    best_reward = float('-inf')
    best_partition = None
    rewards_history = []
    loss_history = []
    variance_history = [] # æ–°å¢

    # æ–°å¢: æŒ‰ episode è®°å½•çš„åˆ—è¡¨
    episode_rewards = []
    episode_variances = []
    
    # è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(range(episodes), desc="è®­ç»ƒPPO")
    for e in progress_bar:
        state, _ = env.reset()
        total_reward = 0
        done = False
        step_rewards = []

        for step in range(max_steps):
            action = agent.act(state)
            next_state, reward, done, _, _ = env.step(action)
            
            agent.store_transition(reward, done)

            state = next_state
            total_reward += reward
            step_rewards.append(reward)

            # è®°å½•å•æ­¥ä¿¡æ¯
            if agent.logger is not None and e % agent.logger.log_freq == 0:
                agent.logger.log_scalar("step/reward", reward, agent.logger.step_count)
                agent.logger.step_count += 1

            if done:
                break
        
        # è®¡ç®—å½“å‰ episode çš„æ–¹å·®
        current_variance = calculate_weight_variance(graph, env.partition_assignment, num_partitions)

        # æ›´æ–°ç­–ç•¥
        loss = agent.update()
        if loss is not None:
            rewards_history.append(total_reward)
            loss_history.append(loss)
            variance_history.append(current_variance)
        
        # æŒ‰ episode è®°å½•æ ¸å¿ƒæŒ‡æ ‡
        episode_rewards.append(total_reward)
        episode_variances.append(current_variance)

        # è®°å½•é¢å¤–æŒ‡æ ‡
        if agent.logger is not None:
            metrics = {
                "performance/weight_variance": current_variance,
                "performance/total_steps": step + 1
            }
            agent.logger.log_metrics(metrics, e)

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'reward': total_reward,
            'best': best_reward,
            'loss': loss_history[-1] if loss_history else 0,
            'variance': variance_history[-1] if variance_history else 0
        })

        # ä¿å­˜æœ€ä½³ç»“æœ
        if total_reward > best_reward:
            best_reward = total_reward
            best_partition = env.partition_assignment.copy()

    # ä¿å­˜æ¨¡å‹
    os.makedirs(f"{results_dir}/models", exist_ok=True)
    agent.save_model(f"{results_dir}/models/ppo_model_{len(graph.nodes())}nodes_{num_partitions}parts.pt")

    return best_partition, rewards_history, loss_history, variance_history, episode_rewards, episode_variances


# æ·»åŠ è®­ç»ƒGNN-PPOæ™ºèƒ½ä½“çš„å‡½æ•°
def train_gnn_ppo_agent(graph, num_partitions, config, results_dir="results"):
    # æ¯ä¸ªç®—æ³•éƒ½ä»ç›¸åŒçš„éšæœºçŠ¶æ€å¼€å§‹
    set_seed(42)
    """è®­ç»ƒGNN-PPOæ™ºèƒ½ä½“"""
    # è·å–é…ç½®å‚æ•°
    episodes = config.get("episodes", 500)
    max_steps = config.get("max_steps", 100)
    gnn_ppo_config = config.get("gnn_ppo_config", {}) # è·å–GNN-PPOé…ç½®
    
    # === åˆ é™¤ï¼šæ‰€æœ‰å¥åº·æ£€æŸ¥ç›¸å…³é…ç½® ===
    # è®¾ç½®åˆ†åŒºæ•°é‡å‚æ•°ï¼Œè¿™æ˜¯æ–°æ¥å£éœ€è¦çš„
    gnn_ppo_config['num_partitions'] = num_partitions

    # --- ä¿®æ”¹ï¼šä½¿ç”¨ new_environment å¹¶ä¼ é€’å‚æ•° ---
    default_potential_weights = {'variance': 1.0, 'edge_cut': 1.0, 'modularity': 1.0}
    potential_weights = config.get("potential_weights", default_potential_weights)
    env = GraphPartitionEnvironment(
        graph,
        num_partitions,
        max_steps,
        gamma=gnn_ppo_config.get('gamma', 0.99), # ä»GNN-PPOé…ç½®è·å–gamma
        potential_weights=potential_weights
    )
    # --- ä¿®æ”¹ç»“æŸ ---

    # === ä¿®æ”¹ï¼šåˆå§‹åŒ–æ–°çš„GNN-PPOä»£ç† ===
    # è®¡ç®—èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ï¼šåˆ†åŒºæ•° + åº¦ + æƒé‡
    node_feature_dim = num_partitions + 2
    action_size = len(graph.nodes()) * num_partitions
    agent = SimplePPOAgentGNN(node_feature_dim, action_size, gnn_ppo_config)
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(f"{results_dir}/models", exist_ok=True)

    best_reward = float('-inf')
    best_partition = None
    rewards_history = []
    loss_history = []
    variance_history = []
    
    # æ–°å¢: æŒ‰ episode è®°å½•çš„åˆ—è¡¨
    episode_rewards = []
    episode_variances = []

    # è®­ç»ƒå¾ªç¯    
    progress_bar = tqdm(range(episodes), desc="è®­ç»ƒGNN-PPO")
    for e in progress_bar:
        # === ä¿®æ”¹ï¼šä½¿ç”¨å›¾ç»“æ„æ•°æ®æ ¼å¼é‡ç½®ç¯å¢ƒ ===
        graph_state, _ = env.reset(state_format='graph')
        total_reward = 0
        
        for step in range(max_steps):
            # === ä¿®æ”¹ï¼šä½¿ç”¨å›¾æ•°æ®è¿›è¡ŒåŠ¨ä½œé€‰æ‹© ===
            action = agent.act(graph_state)
            next_state, reward, done, _, _ = env.step(action)
            
            # === ä¿®æ”¹ï¼šè·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å›¾æ•°æ®æ ¼å¼ ===
            next_graph_state = env.get_state('graph')

            agent.store_transition(reward, done)
            
            graph_state = next_graph_state
            total_reward += reward
            if done:
                break

        # è®¡ç®—å½“å‰ episode çš„æ–¹å·®
        current_variance = np.var(calculate_partition_weights(graph, env.partition_assignment, num_partitions))
        
        # æ›´æ–°ç­–ç•¥
        loss, updated = agent.update()
        
        # ä»…åœ¨æœ‰æ•ˆæ›´æ–°æ—¶è®°å½•
        if updated:
            loss_history.append(loss)
            rewards_history.append(total_reward)
            variance_history.append(current_variance)
        
        # æŒ‰ episode è®°å½•æ ¸å¿ƒæŒ‡æ ‡
        episode_rewards.append(total_reward)
        episode_variances.append(current_variance)

        # === åˆ é™¤ï¼šæ‰€æœ‰å¥åº·æ£€æŸ¥ç›¸å…³ä»£ç  ===
        
        # æ¯50ä¸ªepisodesä¿å­˜ä¸€æ¬¡æ¨¡å‹å¿«ç…§
        if e > 0 and e % 50 == 0:
            snapshot_path = f"{results_dir}/models/gnn_ppo_snapshot_ep{e}.pt"
            agent.save_model(snapshot_path)
            print(f"\nä¿å­˜æ¨¡å‹å¿«ç…§åˆ° {snapshot_path}")

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'reward': total_reward,
            'best': best_reward,
            'loss': loss_history[-1] if loss_history else 0,
            'variance': variance_history[-1] if variance_history else 0
        })

        # ä¿å­˜æœ€ä½³ç»“æœ
        if total_reward > best_reward:
            best_reward = total_reward
            best_partition = env.partition_assignment.copy()

    # === åˆ é™¤ï¼šæ€§èƒ½ç»Ÿè®¡æ‰“å° ===

    # ä¿å­˜æ¨¡å‹
    agent.save_model(f"{results_dir}/models/gnn_ppo_model_{len(graph.nodes())}nodes_{num_partitions}parts.pt")

    return best_partition, rewards_history, loss_history, variance_history, episode_rewards, episode_variances


def run_experiment(graph_name, graph, num_partitions, config, results_dir="results"):
    """è¿è¡Œä¸€æ¬¡å®Œæ•´çš„å®éªŒï¼ŒåŒ…æ‹¬æ‰€æœ‰ç®—æ³•"""
    os.makedirs(results_dir, exist_ok=True)
    os.makedirs(f"{results_dir}/plots", exist_ok=True)

    results = {}
    methods = config.get("methods", ["random", "greedy", "spectral", "metis", "dqn", "gnn", "ppo", "gnn_ppo"])

    # ç”¨äºå­˜å‚¨è®­ç»ƒå†å²
    training_data = {}

    # è¿è¡Œæ¯ç§ç®—æ³•
    for method in methods:
        print(f"\næ‰§è¡Œ {method} ç®—æ³•:")
        start_time = time.time()

        if method == "random":
            partition = random_partition(graph, num_partitions)
        elif method == "greedy":
            partition = weighted_greedy_partition(graph, num_partitions)
        elif method == "spectral":
            partition = spectral_partition(graph, num_partitions)
        elif method == "metis":
            partition = metis_partition(graph, num_partitions)
        elif method == "dqn":
            partition, rewards, losses, variances, episode_rewards, episode_variances = train_dqn_agent(graph, num_partitions, config, results_dir)
            # è®°å½•è®­ç»ƒå†å²
            training_data["dqn"] = {
                "rewards": rewards,
                "loss": losses,
                "variance": variances,
                "episode_rewards": episode_rewards,
                "episode_variances": episode_variances
            }
        elif method == "gnn":
            partition, rewards, losses, variances, episode_rewards, episode_variances = train_gnn_agent(graph, num_partitions, config, results_dir)
            # è®°å½•è®­ç»ƒå†å²
            training_data["gnn"] = {
                "rewards": rewards,
                "loss": losses,
                "variance": variances,
                "episode_rewards": episode_rewards,
                "episode_variances": episode_variances
            }
        elif method == "ppo":
            partition, rewards, losses, variances, episode_rewards, episode_variances = train_ppo_agent(graph, num_partitions, config, results_dir)
            # è®°å½•è®­ç»ƒå†å²
            training_data["ppo"] = {
                "rewards": rewards,
                "loss": losses,
                "variance": variances,
                "episode_rewards": episode_rewards,
                "episode_variances": episode_variances
            }
        elif method == "gnn_ppo":
            partition, rewards, losses, variances, episode_rewards, episode_variances = train_gnn_ppo_agent(graph, num_partitions, config, results_dir)
            # è®°å½•è®­ç»ƒå†å²
            training_data["gnn_ppo"] = {
                "rewards": rewards,
                "loss": losses,
                "variance": variances,
                "episode_rewards": episode_rewards,
                "episode_variances": episode_variances
            }
        else:
            print(f"æœªçŸ¥æ–¹æ³•: {method}")
            continue

        end_time = time.time()
        runtime = end_time - start_time

        # è¯„ä¼°åˆ†åŒºè´¨é‡
        eval_results = evaluate_partition(graph, partition, num_partitions, print_results=True)
        eval_results["runtime"] = runtime
        results[method] = eval_results

    # ä¿å­˜ç»“æœä¸ºDataFrame
    results_df = pd.DataFrame()

    for method, eval_results in results.items():
        method_results = {
            "method": method,
            "weight_variance": eval_results["weight_variance"],
            "weight_imbalance": eval_results["weight_imbalance"],
            "edge_cut": eval_results["edge_cut"],
            "normalized_cut": eval_results["normalized_cut"],
            "modularity": eval_results["modularity"],
            "runtime": eval_results["runtime"]
        }
        results_df = pd.concat([results_df, pd.DataFrame([method_results])], ignore_index=True)

    # ä¿å­˜ç»“æœåˆ°CSV
    results_df.to_csv(f"{results_dir}/{graph_name}_results.csv", index=False)
    print(f"ç»“æœå·²ä¿å­˜åˆ° {results_dir}/{graph_name}_results.csv")

    # ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿å›¾
    if training_data:
        plot_episode_curves(graph_name, training_data, results_dir)
        plot_avg_episode_curves_with_std(graph_name, training_data, results_dir)

    # ç»˜åˆ¶æ¯”è¾ƒå›¾
    plot_comparison(graph_name, results, results_dir)

    return results_df


# æ·»åŠ æ–°çš„ç»˜å›¾å‡½æ•°
def plot_episode_curves(graph_name, training_data, results_dir):
    """ç»˜åˆ¶æŒ‰ episode è®°å½•çš„è®­ç»ƒå†å²æ›²çº¿å›¾"""
    rl_methods = [m for m in training_data.keys()]
    if not rl_methods:
        return

    # ç»˜åˆ¶å¥–åŠ±æ›²çº¿ (æŒ‰ Episode)
    plt.figure(figsize=(12, 8))
    for method in rl_methods:
        rewards = training_data[method].get("episode_rewards")
        if not rewards: continue

        window_size = min(10, len(rewards) // 10) if len(rewards) > 10 else 1
        smoothed_rewards = np.convolve(rewards, np.ones(window_size) / window_size, mode='valid')
        x_axis = np.arange(len(smoothed_rewards)) + (window_size - 1)
        
        plt.plot(x_axis, smoothed_rewards, label=method.upper())
        
    plt.title(f"Episodic Reward Curve - {graph_name}")
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.legend()    
    plt.grid(True)
    plt.savefig(f"{results_dir}/plots/{graph_name}_episodic_rewards.png")
    plt.savefig(f"{results_dir}/plots/{graph_name}_episodic_rewards.svg", format='svg')
    plt.close()

    # ç»˜åˆ¶æ–¹å·®æ›²çº¿ (æŒ‰ Episode)
    plt.figure(figsize=(12, 8))
    for method in rl_methods:
        variances = training_data[method].get("episode_variances")
        if not variances: continue

        window_size = min(10, len(variances) // 10) if len(variances) > 10 else 1
        smoothed_variances = np.convolve(variances, np.ones(window_size) / window_size, mode='valid')
        x_axis = np.arange(len(smoothed_variances)) + (window_size - 1)

        plt.plot(x_axis, smoothed_variances, label=method.upper())
        
    plt.title(f"Episodic Partition Weight Variance Curve - {graph_name}")
    plt.xlabel("Episode")
    plt.ylabel("Weight Variance")
    plt.legend()    
    plt.grid(True)
    plt.savefig(f"{results_dir}/plots/{graph_name}_episodic_variance.png")
    plt.savefig(f"{results_dir}/plots/{graph_name}_episodic_variance.svg", format='svg')
    plt.close()


def plot_avg_episode_curves_with_std(graph_name, training_data, results_dir):
    """ç»˜åˆ¶æŒ‰ episode è®°å½•çš„ã€å¸¦æœ‰æ ‡å‡†å·®é˜´å½±çš„å¹³å‡æ›²çº¿"""
    # åªå¤„ç†æœ‰ episodic æ•°æ®çš„æŒ‡æ ‡
    for data_type in ["rewards", "variances"]:
        plt.figure(figsize=(12, 8))
        
        # æ‰¾å‡ºæœ‰ episodic æ•°æ®çš„ç®—æ³•
        # ä¿®å¤: ä» training_data[method] è·å– episodic æ•°æ®
        methods_with_data = [m for m in training_data.keys() if training_data.get(m, {}).get(f"episode_{data_type}")]
        if not methods_with_data:
            plt.close()
            continue
        
        for method in methods_with_data:
            # ä½¿ç”¨ episodic æ•°æ®
            data = np.array(training_data[method][f"episode_{data_type}"])
            if data.size == 0: continue

            # ä½¿ç”¨ç§»åŠ¨çª—å£è®¡ç®—å¹³å‡å’Œæ ‡å‡†å·®
            window_size = min(30, len(data) // 5) if len(data) > 30 else 1
            if window_size > 1:
                means = np.convolve(data, np.ones(window_size) / window_size, mode='valid')
                stds = np.array([np.std(data[i:i + window_size]) for i in range(len(data) - window_size + 1)])
                
                # Xè½´æ˜¯ episode æ•°
                x_axis = np.arange(len(means)) + (window_size - 1)
                
                plt.plot(x_axis, means, label=method.upper())
                plt.fill_between(x_axis, means - stds, means + stds, alpha=0.3)
            else:
                x_axis = np.arange(len(data))
                plt.plot(x_axis, data, label=method.upper())

        # ä¸ºä¸åŒæ•°æ®ç±»å‹è®¾ç½®é€‚å½“çš„è‹±æ–‡æ ‡é¢˜
        title_mapping = {
            "rewards": "Average Episodic Rewards",
            "variances": "Average Episodic Weight Variance"
        }
        data_type_capitalized = "Reward" if data_type == "rewards" else "Weight Variance"

        plt.title(f"{title_mapping.get(data_type, data_type.capitalize())} Curve (with Std Dev) - {graph_name}")
        plt.xlabel("Episode")
        plt.ylabel(data_type_capitalized)
        plt.legend()        
        plt.grid(True)
        # ä¿å­˜ä¸ºæ–°æ–‡ä»¶å
        plt.savefig(f"{results_dir}/plots/{graph_name}_avg_episodic_{data_type}.png")
        plt.savefig(f"{results_dir}/plots/{graph_name}_avg_episodic_{data_type}.svg", format='svg')
        plt.close()


def plot_comparison(graph_name, results, results_dir):
    """ç»˜åˆ¶ä¸åŒç®—æ³•çš„æ¯”è¾ƒå›¾"""
    # æå–è¯„ä¼°æŒ‡æ ‡
    methods = list(results.keys())
    weight_variance = [results[m]["weight_variance"] for m in methods]
    edge_cut = [results[m]["normalized_cut"] for m in methods]
    execution_time = [results[m]["runtime"] for m in methods]

    # åˆ›å»ºå­å›¾
    fig, axs = plt.subplots(1, 3, figsize=(18, 6))

    # æƒé‡æ–¹å·®
    axs[0].bar(methods, weight_variance)
    axs[0].set_title("Weight Variance")
    axs[0].set_ylabel("Variance")

    # å½’ä¸€åŒ–åˆ‡è¾¹
    axs[1].bar(methods, edge_cut)
    axs[1].set_title("Normalized Edge Cut")
    axs[1].set_ylabel("Ratio")

    # æ‰§è¡Œæ—¶é—´
    axs[2].bar(methods, execution_time)
    axs[2].set_title("Execution Time")
    axs[2].set_ylabel("Seconds")    
    plt.suptitle(f"Algorithm Comparison - {graph_name}")
    plt.tight_layout()
    plt.savefig(f"{results_dir}/plots/{graph_name}_comparison.png")
    plt.savefig(f"{results_dir}/plots/{graph_name}_comparison.svg", format='svg')
    plt.close()


def main():
    # === å…³é”®ä¿®å¤ï¼šè®¾ç½®å…¨å±€éšæœºç§å­ç¡®ä¿å®éªŒå¯é‡å¤æ€§ ===
    SEED = 42
    random.seed(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    # ç¡®ä¿CUDAæ“ä½œçš„ç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸ”§ å…¨å±€éšæœºç§å­è®¾ç½®ä¸º: {SEED} (ç¡®ä¿å®éªŒå¯é‡å¤æ€§)")

    # === GPUä¼˜åŒ–é…ç½® ===
    if torch.cuda.is_available():
        # å¯ç”¨CUDAä¼˜åŒ–
        torch.backends.cudnn.benchmark = True  # é’ˆå¯¹å›ºå®šè¾“å…¥å¤§å°ä¼˜åŒ–
        torch.backends.cuda.matmul.allow_tf32 = True  # å…è®¸TF32åŠ é€Ÿ
        torch.backends.cudnn.allow_tf32 = True
        
        # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
        
        print(f"ğŸš€ GPUä¼˜åŒ–å·²å¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"   - cuDNN benchmark: True")
        print(f"   - TF32 åŠ é€Ÿ: True")
        print(f"   - å†…å­˜åˆ†é…ä¼˜åŒ–: True")
    
    # é™åˆ¶ PyTorch å’Œåº•å±‚åº“ä½¿ç”¨çš„çº¿ç¨‹æ•°ï¼Œå‡å°‘åå°çº¿ç¨‹ç©ºé—²ç­‰å¾…
    # å»ºè®®è®¾ç½®ä¸ºç‰©ç†æ ¸å¿ƒæ•°ï¼Œä¾‹å¦‚ 4 æˆ– 8ï¼Œæ ¹æ®æ‚¨çš„æœåŠ¡å™¨è°ƒæ•´
    num_threads = 6  # ç¨å¾®å‡å°‘é¿å…è¿‡åº¦ç«äº‰
    torch.set_num_threads(num_threads)
    os.environ['OMP_NUM_THREADS'] = str(num_threads)
    os.environ['MKL_NUM_THREADS'] = str(num_threads)
    print(f"ğŸ”§ é™åˆ¶ PyTorch/OMP/MKL çº¿ç¨‹æ•°ä¸º: {num_threads}")

    # æ·»åŠ è¿™ä¸¤è¡Œç¦ç”¨å¼ºåˆ¶åŒæ­¥
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    os.environ['CUDA_FORCE_PTX_JIT'] = '0'

    """ä¸»å‡½æ•°"""
    # === ä¿®æ”¹ï¼šåˆ›å»ºä»¥æ—¶é—´æˆ³å‘½åçš„ç»“æœç›®å½• ===
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"results/{timestamp}"
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs(results_dir, exist_ok=True)
    os.makedirs(f"{results_dir}/plots", exist_ok=True)
    os.makedirs(f"{results_dir}/models", exist_ok=True)
    
    print(f"è®­ç»ƒå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ç»“æœå°†ä¿å­˜åˆ°: {results_dir}")

    # é»˜è®¤é…ç½®
    default_config = {
        "episodes": 500,
        "max_steps": 100,
        "batch_size": 32,
        "methods": ["random", "greedy", "spectral", "metis", "dqn", "gnn", "ppo", "gnn_ppo"],
        "dqn_config": {
            "gamma": 0.95,
            "epsilon": 1.0,
            "epsilon_min": 0.01,
            "epsilon_decay": 0.995,
            "learning_rate": 0.001,
            "target_update_freq": 10
        },
        "gnn_config": {
            "gamma": 0.95,
            "epsilon": 1.0,
            "epsilon_min": 0.01,
            "epsilon_decay": 0.995,
            "learning_rate": 0.001,
            "hidden_dim": 128,
            "target_update_freq": 10
        },
        "ppo_config": {
            "gamma": 0.99,
            "learning_rate": 0.0003,
            "ppo_epochs": 4,
            "batch_size": 64,
            "clip_ratio": 0.2,
            "entropy_coef": 0.01,
            "value_coef": 0.5
        },
        "gnn_ppo_config": {
            "gamma": 0.99,
            "learning_rate": 0.0003,
            "ppo_epochs": 4,
            "batch_size": 64,
            "clip_ratio": 0.2,
            "entropy_coef": 0.01,
            "value_coef": 0.5,
            "hidden_dim": 128,
            "gnn_layers": 2
        }
    }

    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
    if not os.path.exists("configs/default.json"):
        with open("configs/default.json", "w") as f:
            json.dump(default_config, f, indent=4)

    # åŠ è½½é…ç½®
    config = load_config("configs/default.json")

    # === ä¿®æ”¹ï¼šä¼˜å…ˆå°è¯•åŠ è½½çœŸå®å›¾ï¼Œå¦åˆ™ä½¿ç”¨æµ‹è¯•å›¾ ===
    real_graph_path = "ctu_airspace_graph_1900_2000_kmeans.graphml"
    
    if os.path.exists(real_graph_path):
        print(f"ğŸ”„ ä½¿ç”¨çœŸå®ç©ºåŸŸå›¾: {real_graph_path}")
        try:
            graph = nx.read_graphml(real_graph_path)
            
            # é‡æ–°ç¼–å·èŠ‚ç‚¹ç¡®ä¿è¿ç»­æ€§
            node_mapping = {old_node: i for i, old_node in enumerate(graph.nodes())}
            graph = nx.relabel_nodes(graph, node_mapping)
            
            # ç¡®ä¿æƒé‡ä¸ºæ•°å€¼ç±»å‹
            for node in graph.nodes():
                if 'weight' in graph.nodes[node]:
                    graph.nodes[node]['weight'] = float(graph.nodes[node]['weight'])
                else:
                    graph.nodes[node]['weight'] = 1.0
            
            # å¦‚æœæ²¡æœ‰è¾¹ï¼Œæ·»åŠ ç®€å•çš„è¿æ¥
            if graph.number_of_edges() == 0:
                print("âš ï¸  å›¾æ²¡æœ‰è¾¹ï¼Œæ·»åŠ åŸºæœ¬è¿æ¥...")
                nodes = list(graph.nodes())
                for i in range(len(nodes) - 1):
                    graph.add_edge(nodes[i], nodes[i + 1])
                # æ·»åŠ ä¸€äº›éšæœºè¿æ¥
                for _ in range(min(50, len(nodes) * 2)):
                    u, v = random.choice(nodes), random.choice(nodes)
                    if u != v:
                        graph.add_edge(u, v)
            
            num_partitions = 3 if graph.number_of_nodes() > 15 else 2
            graph_name = f"real_airspace_{graph.number_of_nodes()}nodes"
            
            print(f"âœ… çœŸå®å›¾åŠ è½½æˆåŠŸ: {graph.number_of_nodes()}èŠ‚ç‚¹, {graph.number_of_edges()}è¾¹, {num_partitions}åˆ†åŒº")
            
        except Exception as e:
            print(f"âŒ çœŸå®å›¾åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°æµ‹è¯•å›¾...")
            graph = create_test_graph(num_nodes=10, seed=42)
            num_partitions = 2
            graph_name = "test_graph_10"
    else:
        print(f"ğŸ”„ çœŸå®å›¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨æµ‹è¯•å›¾")
        graph = create_test_graph(num_nodes=10, seed=42)
        num_partitions = 2
        graph_name = "test_graph_10"

    print("å¼€å§‹å›¾åˆ’åˆ†å®éªŒ...")
    df = run_experiment(graph_name, graph, num_partitions, config, results_dir)

    # === æ–°å¢ï¼šåˆ›å»ºè®­ç»ƒä¿¡æ¯è®°å½•æ–‡ä»¶ ===
    training_info = {
        "è®­ç»ƒå¼€å§‹æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "è®­ç»ƒå®Œæˆæ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "å›¾èŠ‚ç‚¹æ•°": len(graph.nodes()),
        "å›¾è¾¹æ•°": len(graph.edges()),
        "åˆ†åŒºæ•°": num_partitions,
        "è®­ç»ƒé…ç½®": config,
        "æœ€ç»ˆç»“æœ": df.to_dict('records')
    }
    
    import json
    with open(f"{results_dir}/training_info.json", "w", encoding='utf-8') as f:
        json.dump(training_info, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºç®€æ´çš„READMEæ–‡ä»¶
    readme_content = f"""# å›¾åˆ’åˆ†å®éªŒç»“æœ

## è®­ç»ƒä¿¡æ¯
- **å¼€å§‹æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **å›¾è§„æ¨¡**: {len(graph.nodes())} ä¸ªèŠ‚ç‚¹, {len(graph.edges())} æ¡è¾¹
- **åˆ†åŒºæ•°**: {num_partitions}
- **è®­ç»ƒæ–¹æ³•**: {', '.join(config.get('methods', []))}

## æ–‡ä»¶è¯´æ˜
- `plots/`: åŒ…å«æ‰€æœ‰è®­ç»ƒæ›²çº¿å’Œæ¯”è¾ƒå›¾è¡¨
- `models/`: åŒ…å«è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶
- `*.csv`: å®éªŒç»“æœæ•°æ®
- `training_info.json`: è¯¦ç»†çš„è®­ç»ƒé…ç½®å’Œç»“æœ

## æœ€ä½³ç»“æœé¢„è§ˆ
{df.to_string(index=False)}
"""
    
    with open(f"{results_dir}/README.md", "w", encoding='utf-8') as f:
        f.write(readme_content)

    print(f"\nå®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {results_dir} ç›®å½•")
    print(f"æŸ¥çœ‹ {results_dir}/README.md äº†è§£è¯¦ç»†ä¿¡æ¯")
    print(df)



if __name__ == "__main__":
    main()