# run experiments
import os
import time
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
import json
import torch
from tqdm import tqdm

# é…ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
try:
    # å°è¯•ä½¿ç”¨æ–‡æ³‰é©¿å¾®ç±³é»‘å­—ä½“
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'SimHei', 'DejaVu Sans', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
    
    # éªŒè¯å­—ä½“åŠ è½½
    plt.rcParams['font.family'] = 'sans-serif'  # ä½¿ç”¨æ— è¡¬çº¿å­—ä½“
except Exception as e:
    print(f"å­—ä½“é…ç½®é”™è¯¯: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ä½“")

from new_environment import GraphPartitionEnvironment
from agent_dqn_basic import DQNAgent
from metrics import evaluate_partition
from baselines import random_partition, weighted_greedy_partition, metis_partition, spectral_partition
from agent_gnn import GNNDQNAgent
from agent_ppo import PPOAgent
from agent_ppo_gnn_simple import SimplePPOAgentGNN
from metrics import calculate_weight_variance, calculate_partition_weights

def create_test_graph(num_nodes=10, seed=42):
    """åˆ›å»ºæµ‹è¯•å›¾ï¼Œå¸¦æœ‰èŠ‚ç‚¹æƒé‡"""
    np.random.seed(seed)
    G = nx.random_geometric_graph(num_nodes, 0.5, seed=seed)

    # æ·»åŠ èŠ‚ç‚¹æƒé‡
    for i in range(num_nodes):
        G.nodes[i]['weight'] = np.random.randint(1, 10)

    return G


def load_graph_from_file(filepath):
    """ä»æ–‡ä»¶åŠ è½½å›¾"""
    try:
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½æ–¹æ³•
        extension = os.path.splitext(filepath)[1]
        if extension == '.graphml':
            G = nx.read_graphml(filepath)
        elif extension == '.gexf':
            G = nx.read_gexf(filepath)
        elif extension == '.edgelist':
            G = nx.read_edgelist(filepath)
        else:
            # é»˜è®¤å°è¯•pickleåŠ è½½
            G = nx.read_gpickle(filepath)

        # å¦‚æœæ²¡æœ‰èŠ‚ç‚¹æƒé‡ï¼Œæ·»åŠ é»˜è®¤æƒé‡1
        for node in G.nodes():
            if 'weight' not in G.nodes[node]:
                G.nodes[node]['weight'] = 1

        return G
    except Exception as e:
        print(f"åŠ è½½å›¾æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None


def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return {}


def train_dqn_agent(graph, num_partitions, config, results_dir="results"):
    """è®­ç»ƒDQNæ™ºèƒ½ä½“"""
    # è·å–é…ç½®å‚æ•°
    episodes = config.get("episodes", 1000)
    max_steps = config.get("max_steps", 100)
    batch_size = config.get("dqn_config", {}).get("batch_size", config.get("batch_size", 32))
    dqn_config = config.get("dqn_config", {}) # è·å–DQNé…ç½®

    # --- ä¿®æ”¹ï¼šä½¿ç”¨ new_environment å¹¶ä¼ é€’å‚æ•° ---
    # ä½¿ç”¨é»˜è®¤çš„ potential_weights æˆ–ä»é…ç½®åŠ è½½
    default_potential_weights = {'variance': 1.0, 'edge_cut': 1.0, 'modularity': 1.0}
    potential_weights = config.get("potential_weights", default_potential_weights)
    env = GraphPartitionEnvironment(
        graph,
        num_partitions,
        max_steps,
        gamma=dqn_config.get('gamma', 0.95), # ä»DQNé…ç½®è·å–gamma
        potential_weights=potential_weights
    )

    # åˆå§‹åŒ–DQNä»£ç†
    num_nodes = len(graph.nodes())
    # --- ä¿®æ”¹ï¼šæ›´æ–°çŠ¶æ€å¤§å°è®¡ç®— (+1 for node weights) ---
    state_size = num_nodes * (num_partitions + 2)
    # --- ä¿®æ”¹ç»“æŸ ---
    action_size = num_nodes * num_partitions
    dqn_config['batch_size'] = batch_size # ä¼ é€’æ­£ç¡®çš„batch_sizeç»™agent
    agent = DQNAgent(state_size, action_size, dqn_config)

    best_reward = float('-inf')
    best_partition = None
    rewards_history = []
    loss_history = []  # æ–°å¢ï¼šè®°å½•æŸå¤±å†å²
    variance_history = []  # æ–°å¢ï¼šè®°å½•æ–¹å·®å†å²

    # è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(range(episodes), desc="è®­ç»ƒDQN")
    for e in progress_bar:
        state, _ = env.reset()
        total_reward = 0
        done = False

        for step in range(max_steps):
            action = agent.act(state)
            next_state, reward, done, _, info = env.step(action)

            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

            if done:
                break
        
        # ä½¿ç”¨ agent.memory_counter è·å–å®é™…å­˜å‚¨çš„æ ·æœ¬æ•°
        memory_size = min(agent.memory_counter, agent.memory_capacity)

        # è¿›è¡Œç»éªŒå›æ”¾å­¦ä¹ 
        if memory_size >= batch_size:
            loss = agent.replay(batch_size)  # è·å–è¿”å›çš„æŸå¤±å€¼
            loss_history.append(loss)  # è®°å½•æŸå¤±
        else:
            loss_history.append(0.0)  # å¦‚æœæ²¡æœ‰å­¦ä¹ ï¼Œè®°å½•0æŸå¤±

        # è®¡ç®—å¹¶è®°å½•å½“å‰åˆ†åŒºçš„æ–¹å·®
        if env.partition_assignment is not None:
            from metrics import calculate_weight_variance
            variance = calculate_weight_variance(graph, env.partition_assignment, num_partitions)
            variance_history.append(variance)  # è®°å½•æ–¹å·®
        else:
            variance_history.append(0.0)  # é»˜è®¤æ–¹å·®ä¸º0

        # è®°å½•å¥–åŠ±
        rewards_history.append(total_reward)

        # æ›´æ–°æœ€ä½³åˆ’åˆ†
        if total_reward > best_reward:
            best_reward = total_reward
            best_partition = env.partition_assignment.copy()

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'reward': total_reward,
            'best': best_reward,
            'epsilon': agent.epsilon,
            'loss': loss_history[-1] if loss_history else 0,
            'variance': variance_history[-1] if variance_history else 0
        })

        # æ¯50ä¸ªepisodeä¿å­˜ä¸€æ¬¡æ¨¡å‹
        if (e + 1) % 50 == 0:
            os.makedirs(f"{results_dir}/models", exist_ok=True)
            agent.save_model(f"{results_dir}/models/dqn_model_{len(graph.nodes())}nodes_{num_partitions}parts_temp.pt")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    os.makedirs(f"{results_dir}/models", exist_ok=True)
    agent.save_model(f"{results_dir}/models/dqn_model_{len(graph.nodes())}nodes_{num_partitions}parts.pt")

    return best_partition, rewards_history, loss_history, variance_history


def train_gnn_agent(graph, num_partitions, config, results_dir="results"):
    """è®­ç»ƒGNNæ™ºèƒ½ä½“"""
    # è·å–é…ç½®å‚æ•°
    episodes = config.get("episodes", 500)
    max_steps = config.get("max_steps", 100)
    batch_size = config.get("batch_size", 32)
    gnn_config = config.get("gnn_config", {}) # è·å–GNNé…ç½®

    # --- ä¿®æ”¹ï¼šä½¿ç”¨ new_environment å¹¶ä¼ é€’å‚æ•° ---
    default_potential_weights = {'variance': 1.0, 'edge_cut': 1.0, 'modularity': 1.0}
    potential_weights = config.get("potential_weights", default_potential_weights)
    env = GraphPartitionEnvironment(
        graph,
        num_partitions,
        max_steps,
        gamma=gnn_config.get('gamma', 0.95), # ä»GNNé…ç½®è·å–gamma
        potential_weights=potential_weights
    )
    # --- ä¿®æ”¹ç»“æŸ ---

    # åˆå§‹åŒ–GNN-DQNä»£ç†
    agent = GNNDQNAgent(graph, num_partitions, gnn_config)

    best_reward = float('-inf')
    best_partition = None
    rewards_history = []
    loss_history = []  # æ–°å¢ï¼šè®°å½•æŸå¤±å†å²
    variance_history = []  # æ–°å¢ï¼šè®°å½•æ–¹å·®å†å²

    # è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(range(episodes), desc="è®­ç»ƒGNN-DQN")
    for e in progress_bar:
        state, _ = env.reset()
        total_reward = 0
        done = False

        for step in range(max_steps):
            action = agent.act(state)
            next_state, reward, done, _, info = env.step(action)

            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

            if done:
                break

        # æ£€æŸ¥å†…å­˜æ± å¤§å° - ä½¿ç”¨memory_counterè€Œä¸æ˜¯len(agent.memory)
        memory_size = min(agent.memory_counter, agent.memory_capacity)

        # è¿›è¡Œç»éªŒå›æ”¾å­¦ä¹ 
        if memory_size >= batch_size:
            loss = agent.replay()  # è·å–è¿”å›çš„æŸå¤±å€¼
            loss_history.append(loss)  # è®°å½•æŸå¤±
        else:
            loss_history.append(0.0)  # å¦‚æœæ²¡æœ‰å­¦ä¹ ï¼Œè®°å½•0æŸå¤±

        # è®¡ç®—å¹¶è®°å½•å½“å‰åˆ†åŒºçš„æ–¹å·®
        if env.partition_assignment is not None:
            from metrics import calculate_weight_variance
            variance = calculate_weight_variance(graph, env.partition_assignment, num_partitions)
            variance_history.append(variance)  # è®°å½•æ–¹å·®
        else:
            variance_history.append(0.0)  # é»˜è®¤æ–¹å·®ä¸º0

        # è®°å½•å¥–åŠ±
        rewards_history.append(total_reward)

        # æ›´æ–°æœ€ä½³åˆ’åˆ†
        if total_reward > best_reward:
            best_reward = total_reward
            best_partition = env.partition_assignment.copy()

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'reward': total_reward,
            'best': best_reward,
            'epsilon': agent.epsilon,
            'loss': loss_history[-1] if loss_history else 0,
            'variance': variance_history[-1] if variance_history else 0
        })

        # æ¯10ä¸ªepisodeä¿å­˜ä¸€æ¬¡æ¨¡å‹
        if (e + 1) % 50 == 0:
            os.makedirs(f"{results_dir}/models", exist_ok=True)
            agent.save_model(f"{results_dir}/models/gnn_model_{len(graph.nodes())}nodes_{num_partitions}parts_temp.pt")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    os.makedirs(f"{results_dir}/models", exist_ok=True)
    agent.save_model(f"{results_dir}/models/gnn_model_{len(graph.nodes())}nodes_{num_partitions}parts.pt")

    return best_partition, rewards_history, loss_history, variance_history

# æ·»åŠ è®­ç»ƒPPOæ™ºèƒ½ä½“çš„å‡½æ•°
def train_ppo_agent(graph, num_partitions, config, results_dir="results"):
    """è®­ç»ƒPPOæ™ºèƒ½ä½“"""
    # è·å–é…ç½®å‚æ•°
    episodes = config.get("episodes", 1000)
    max_steps = config.get("max_steps", 100)
    ppo_config = config.get("ppo_config", {}) # è·å–PPOé…ç½®

    # --- ä¿®æ”¹ï¼šä½¿ç”¨ new_environment å¹¶ä¼ é€’å‚æ•° ---
    default_potential_weights = {'variance': 1.0, 'edge_cut': 1.0, 'modularity': 1.0}
    potential_weights = config.get("potential_weights", default_potential_weights)
    env = GraphPartitionEnvironment(
        graph,
        num_partitions,
        max_steps,
        gamma=ppo_config.get('gamma', 0.99), # ä»PPOé…ç½®è·å–gamma
        potential_weights=potential_weights
    )
    # --- ä¿®æ”¹ç»“æŸ ---

    # åˆå§‹åŒ–PPOä»£ç†
    num_nodes = len(graph.nodes())
    # --- ä¿®æ”¹ï¼šæ›´æ–°çŠ¶æ€å¤§å°è®¡ç®— (+1 for node weights) ---
    state_size = num_nodes * (num_partitions + 2)
    # --- ä¿®æ”¹ç»“æŸ ---
    action_size = num_nodes * num_partitions
    agent = PPOAgent(state_size, action_size, ppo_config)

    best_reward = float('-inf')
    best_partition = None
    rewards_history = []
    loss_history = []  # æ–°å¢ï¼šè®°å½•æŸå¤±
    variance_history = []  # æ–°å¢ï¼šè®°å½•æ–¹å·®

    # è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(range(episodes), desc="è®­ç»ƒPPO")
    for e in progress_bar:
        state, _ = env.reset()
        total_reward = 0
        step_rewards = []

        for step in range(max_steps):
            action = agent.act(state)
            next_state, reward, done, _, _ = env.step(action)

            agent.store_transition(reward, done)
            state = next_state
            total_reward += reward
            step_rewards.append(reward)

            # è®°å½•å•æ­¥ä¿¡æ¯
            if agent.logger is not None and e % agent.logger.log_freq == 0:
                agent.logger.log_scalar("step/reward", reward, agent.logger.step_count)
                agent.logger.step_count += 1

            if done:
                break

        # æ›´æ–°ç­–ç•¥
        loss = agent.update()
        rewards_history.append(total_reward)
        loss_history.append(loss)

        # è®¡ç®—å½“å‰åˆ†åŒºæƒé‡æ–¹å·®
        partition_weights = calculate_partition_weights(graph, env.partition_assignment, num_partitions)
        weight_variance = np.var(partition_weights)
        variance_history.append(weight_variance)

        # è®°å½•é¢å¤–æŒ‡æ ‡
        if agent.logger is not None:
            metrics = {
                "performance/weight_variance": weight_variance,
                "performance/total_steps": step + 1
            }
            agent.logger.log_metrics(metrics, e)

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'reward': total_reward,
            'best': best_reward,
            'loss': loss_history[-1] if loss_history else 0,
            'variance': variance_history[-1] if variance_history else 0
        })

        # ä¿å­˜æœ€ä½³ç»“æœ
        if total_reward > best_reward:
            best_reward = total_reward
            best_partition = env.partition_assignment.copy()

    # ä¿å­˜æ¨¡å‹
    os.makedirs(f"{results_dir}/models", exist_ok=True)
    agent.save_model(f"{results_dir}/models/ppo_model_{len(graph.nodes())}nodes_{num_partitions}parts.pt")

    return best_partition, rewards_history, loss_history, variance_history


# æ·»åŠ è®­ç»ƒGNN-PPOæ™ºèƒ½ä½“çš„å‡½æ•°
def train_gnn_ppo_agent(graph, num_partitions, config, results_dir="results"):
    """è®­ç»ƒGNN-PPOæ™ºèƒ½ä½“"""
    # è·å–é…ç½®å‚æ•°
    episodes = config.get("episodes", 500)
    max_steps = config.get("max_steps", 100)
    gnn_ppo_config = config.get("gnn_ppo_config", {}) # è·å–GNN-PPOé…ç½®
    
    # === åˆ é™¤ï¼šæ‰€æœ‰å¥åº·æ£€æŸ¥ç›¸å…³é…ç½® ===
    # è®¾ç½®åˆ†åŒºæ•°é‡å‚æ•°ï¼Œè¿™æ˜¯æ–°æ¥å£éœ€è¦çš„
    gnn_ppo_config['num_partitions'] = num_partitions

    # --- ä¿®æ”¹ï¼šä½¿ç”¨ new_environment å¹¶ä¼ é€’å‚æ•° ---
    default_potential_weights = {'variance': 1.0, 'edge_cut': 1.0, 'modularity': 1.0}
    potential_weights = config.get("potential_weights", default_potential_weights)
    env = GraphPartitionEnvironment(
        graph,
        num_partitions,
        max_steps,
        gamma=gnn_ppo_config.get('gamma', 0.99), # ä»GNN-PPOé…ç½®è·å–gamma
        potential_weights=potential_weights
    )
    # --- ä¿®æ”¹ç»“æŸ ---

    # === ä¿®æ”¹ï¼šåˆå§‹åŒ–æ–°çš„GNN-PPOä»£ç† ===
    # è®¡ç®—èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ï¼šåˆ†åŒºæ•° + åº¦ + æƒé‡
    node_feature_dim = num_partitions + 2
    action_size = len(graph.nodes()) * num_partitions
    agent = SimplePPOAgentGNN(node_feature_dim, action_size, gnn_ppo_config)
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(f"{results_dir}/models", exist_ok=True)

    best_reward = float('-inf')
    best_partition = None
    rewards_history = []
    loss_history = []
    variance_history = []
    
    # è®­ç»ƒå¾ªç¯    
    progress_bar = tqdm(range(episodes), desc="è®­ç»ƒGNN-PPO")
    for e in progress_bar:
        # === ä¿®æ”¹ï¼šä½¿ç”¨å›¾ç»“æ„æ•°æ®æ ¼å¼é‡ç½®ç¯å¢ƒ ===
        graph_state, _ = env.reset(state_format='graph')
        total_reward = 0
        
        for step in range(max_steps):
            # === ä¿®æ”¹ï¼šä½¿ç”¨å›¾æ•°æ®è¿›è¡ŒåŠ¨ä½œé€‰æ‹© ===
            action = agent.act(graph_state)
            next_state, reward, done, _, _ = env.step(action)
            
            # === ä¿®æ”¹ï¼šè·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å›¾æ•°æ®æ ¼å¼ ===
            next_graph_state = env.get_state('graph')

            agent.store_transition(reward, done)
            
            graph_state = next_graph_state
            total_reward += reward
            if done:
                break

        # æ›´æ–°ç­–ç•¥
        loss = agent.update()
        loss_history.append(loss)
        rewards_history.append(total_reward)

        # è®¡ç®—å½“å‰åˆ†åŒºæƒé‡æ–¹å·®
        partition_weights = calculate_partition_weights(graph, env.partition_assignment, num_partitions)
        weight_variance = np.var(partition_weights)
        variance_history.append(weight_variance)
        
        # === åˆ é™¤ï¼šæ‰€æœ‰å¥åº·æ£€æŸ¥ç›¸å…³ä»£ç  ===
        
        # æ¯50ä¸ªepisodesä¿å­˜ä¸€æ¬¡æ¨¡å‹å¿«ç…§
        if e > 0 and e % 50 == 0:
            snapshot_path = f"{results_dir}/models/gnn_ppo_snapshot_ep{e}.pt"
            agent.save_model(snapshot_path)
            print(f"\nä¿å­˜æ¨¡å‹å¿«ç…§åˆ° {snapshot_path}")

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'reward': total_reward,
            'best': best_reward,
            'loss': loss_history[-1] if loss_history else 0,
            'variance': variance_history[-1] if variance_history else 0
        })

        # ä¿å­˜æœ€ä½³ç»“æœ
        if total_reward > best_reward:
            best_reward = total_reward
            best_partition = env.partition_assignment.copy()

    # === åˆ é™¤ï¼šæ€§èƒ½ç»Ÿè®¡æ‰“å° ===

    # ä¿å­˜æ¨¡å‹
    agent.save_model(f"{results_dir}/models/gnn_ppo_model_{len(graph.nodes())}nodes_{num_partitions}parts.pt")

    return best_partition, rewards_history, loss_history, variance_history


def run_experiment(graph_name, graph, num_partitions, config, results_dir="results"):
    """è¿è¡Œä¸€æ¬¡å®Œæ•´çš„å®éªŒï¼ŒåŒ…æ‹¬æ‰€æœ‰ç®—æ³•"""
    os.makedirs(results_dir, exist_ok=True)
    os.makedirs(f"{results_dir}/plots", exist_ok=True)

    results = {}
    methods = config.get("methods", ["random", "greedy", "spectral", "metis", "dqn", "gnn", "ppo", "gnn_ppo"])

    # ç”¨äºå­˜å‚¨è®­ç»ƒå†å²
    training_data = {}

    # è¿è¡Œæ¯ç§ç®—æ³•
    for method in methods:
        print(f"\næ‰§è¡Œ {method} ç®—æ³•:")
        start_time = time.time()

        if method == "random":
            partition = random_partition(graph, num_partitions)
        elif method == "greedy":
            partition = weighted_greedy_partition(graph, num_partitions)
        elif method == "spectral":
            partition = spectral_partition(graph, num_partitions)
        elif method == "metis":
            partition = metis_partition(graph, num_partitions)
        elif method == "dqn":
            partition, rewards, losses, variances = train_dqn_agent(graph, num_partitions, config, results_dir)
            # è®°å½•è®­ç»ƒå†å²
            training_data["dqn"] = {
                "rewards": rewards,
                "loss": losses,
                "variance": variances
            }
        elif method == "gnn":
            partition, rewards, losses, variances = train_gnn_agent(graph, num_partitions, config, results_dir)
            # è®°å½•è®­ç»ƒå†å²
            training_data["gnn"] = {
                "rewards": rewards,
                "loss": losses,
                "variance": variances
            }
        elif method == "ppo":
            partition, rewards, losses, variances = train_ppo_agent(graph, num_partitions, config, results_dir)
            # è®°å½•è®­ç»ƒå†å²
            training_data["ppo"] = {
                "rewards": rewards,
                "loss": losses,
                "variance": variances
            }
        elif method == "gnn_ppo":
            partition, rewards, losses, variances = train_gnn_ppo_agent(graph, num_partitions, config, results_dir)
            # è®°å½•è®­ç»ƒå†å²
            training_data["gnn_ppo"] = {
                "rewards": rewards,
                "loss": losses,
                "variance": variances
            }
        else:
            print(f"æœªçŸ¥æ–¹æ³•: {method}")
            continue

        end_time = time.time()
        runtime = end_time - start_time

        # è¯„ä¼°åˆ†åŒºè´¨é‡
        eval_results = evaluate_partition(graph, partition, num_partitions, print_results=True)
        eval_results["runtime"] = runtime
        results[method] = eval_results

    # ä¿å­˜ç»“æœä¸ºDataFrame
    results_df = pd.DataFrame()

    for method, eval_results in results.items():
        method_results = {
            "method": method,
            "weight_variance": eval_results["weight_variance"],
            "weight_imbalance": eval_results["weight_imbalance"],
            "edge_cut": eval_results["edge_cut"],
            "normalized_cut": eval_results["normalized_cut"],
            "modularity": eval_results["modularity"],
            "runtime": eval_results["runtime"]
        }
        results_df = pd.concat([results_df, pd.DataFrame([method_results])], ignore_index=True)

    # ä¿å­˜ç»“æœåˆ°CSV
    results_df.to_csv(f"{results_dir}/{graph_name}_results.csv", index=False)
    print(f"ç»“æœå·²ä¿å­˜åˆ° {results_dir}/{graph_name}_results.csv")

    # ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿å›¾
    if training_data:
        plot_training_curves(graph_name, training_data, results_dir)

    # ç»˜åˆ¶æ¯”è¾ƒå›¾
    plot_comparison(graph_name, results, results_dir)

    return results_df


# æ·»åŠ æ–°çš„ç»˜å›¾å‡½æ•°
def plot_training_curves(graph_name, training_data, results_dir):
    """ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿å›¾"""
    # åªå¤„ç†æœ‰è®­ç»ƒå†å²çš„ç®—æ³•
    rl_methods = [m for m in training_data.keys()]
    if not rl_methods:
        return

    # ç»˜åˆ¶å¥–åŠ±æ›²çº¿
    plt.figure(figsize=(12, 8))
    for method in rl_methods:
        rewards = training_data[method]["rewards"]
        # å¹³æ»‘å¤„ç†ä»¥ä¾¿æ›´å¥½åœ°å¯è§†åŒ–
        window_size = min(10, len(rewards) // 10) if len(rewards) > 10 else 1
        smoothed_rewards = np.convolve(rewards, np.ones(window_size) / window_size, mode='valid')
        plt.plot(smoothed_rewards, label=method.upper())
    plt.title(f"Training Reward Curve - {graph_name}")
    plt.xlabel("Episodes")
    plt.ylabel("Total Reward")
    plt.legend()    
    plt.grid(True)
    plt.savefig(f"{results_dir}/plots/{graph_name}_rewards_comparison.png")
    plt.savefig(f"{results_dir}/plots/{graph_name}_rewards_comparison.svg", format='svg')
    plt.close()

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(12, 8))
    for method in rl_methods:
        loss = training_data[method]["loss"]
        # å¹³æ»‘å¤„ç†ä»¥ä¾¿æ›´å¥½åœ°å¯è§†åŒ–
        window_size = min(10, len(loss) // 10) if len(loss) > 10 else 1
        smoothed_loss = np.convolve(loss, np.ones(window_size) / window_size, mode='valid')
        plt.plot(smoothed_loss, label=method.upper())
    plt.title(f"Training Loss Curve - {graph_name}")
    plt.xlabel("Episodes")
    plt.ylabel("Loss")
    plt.legend()    
    plt.grid(True)
    plt.savefig(f"{results_dir}/plots/{graph_name}_loss_comparison.png")
    plt.savefig(f"{results_dir}/plots/{graph_name}_loss_comparison.svg", format='svg')
    plt.close()

    # ç»˜åˆ¶æ–¹å·®æ›²çº¿
    plt.figure(figsize=(12, 8))
    for method in rl_methods:
        variance = training_data[method]["variance"]
        # å¹³æ»‘å¤„ç†ä»¥ä¾¿æ›´å¥½åœ°å¯è§†åŒ–
        window_size = min(10, len(variance) // 10) if len(variance) > 10 else 1
        smoothed_variance = np.convolve(variance, np.ones(window_size) / window_size, mode='valid')
        plt.plot(smoothed_variance, label=method.upper())
    plt.title(f"Partition Weight Variance Curve - {graph_name}")
    plt.xlabel("Episodes")
    plt.ylabel("Weight Variance")
    plt.legend()    
    plt.grid(True)
    plt.savefig(f"{results_dir}/plots/{graph_name}_variance_comparison.png")
    plt.savefig(f"{results_dir}/plots/{graph_name}_variance_comparison.svg", format='svg')
    plt.close()

    # ç»˜åˆ¶å¸¦æœ‰æ ‡å‡†å·®é˜´å½±çš„å¹³å‡æ›²çº¿
    plot_avg_curves_with_std(graph_name, training_data, results_dir)


def plot_avg_curves_with_std(graph_name, training_data, results_dir):
    """ç»˜åˆ¶å¸¦æœ‰æ ‡å‡†å·®é˜´å½±çš„å¹³å‡æ›²çº¿"""
    # å¯¹æ¯ç§æ•°æ®ç±»å‹åˆ†åˆ«ç»˜å›¾
    for data_type in ["rewards", "loss", "variance"]:
        plt.figure(figsize=(12, 8))

        # æ‰¾å‡ºæœ€å°é•¿åº¦ä»¥ä¾¿å¯¹é½æ•°æ®
        min_length = min(len(training_data[method][data_type]) for method in training_data.keys())

        for method in training_data.keys():
            data = np.array(training_data[method][data_type][:min_length])

            # å¦‚æœæœ‰å¤šæ¬¡è¿è¡Œç»“æœï¼Œè®¡ç®—å¹³å‡å’Œæ ‡å‡†å·®
            # è¿™é‡Œå‡è®¾æˆ‘ä»¬åªæœ‰å•æ¬¡è¿è¡Œï¼Œæ‰€ä»¥æ¨¡æ‹Ÿå¤šæ¬¡è¿è¡Œé€šè¿‡ç§»åŠ¨çª—å£
            window_size = min(30, len(data) // 5) if len(data) > 30 else 1
            if window_size > 1:
                # ä½¿ç”¨ç§»åŠ¨çª—å£è®¡ç®—å¹³å‡å’Œæ ‡å‡†å·®
                means = np.convolve(data, np.ones(window_size) / window_size, mode='valid')

                # è®¡ç®—ç§»åŠ¨æ ‡å‡†å·®
                stds = []
                for i in range(len(data) - window_size + 1):
                    stds.append(np.std(data[i:i + window_size]))
                stds = np.array(stds)

                x = np.arange(len(means))
                plt.plot(x, means, label=method.upper())
                plt.fill_between(x, means - stds, means + stds, alpha=0.3)
            else:
                # æ•°æ®å¤ªå°‘ï¼Œç›´æ¥ç»˜åˆ¶
                plt.plot(data, label=method.upper())

        # ä¸ºä¸åŒæ•°æ®ç±»å‹è®¾ç½®é€‚å½“çš„è‹±æ–‡æ ‡é¢˜
        title_mapping = {
            "rewards": "Average Rewards",
            "loss": "Average Loss",
            "variance": "Average Weight Variance"
        }
        plt.title(f"{title_mapping.get(data_type, data_type.capitalize())} Curve (with Std Dev) - {graph_name}")
        plt.xlabel("Episodes")
        plt.ylabel(data_type.capitalize())
        plt.legend()        
        plt.grid(True)
        plt.savefig(f"{results_dir}/plots/{graph_name}_avg_{data_type}.png")
        plt.savefig(f"{results_dir}/plots/{graph_name}_avg_{data_type}.svg", format='svg')
        plt.close()



def plot_comparison(graph_name, results, results_dir):
    """ç»˜åˆ¶ä¸åŒç®—æ³•çš„æ¯”è¾ƒå›¾"""
    # æå–è¯„ä¼°æŒ‡æ ‡
    methods = list(results.keys())
    weight_variance = [results[m]["weight_variance"] for m in methods]
    edge_cut = [results[m]["normalized_cut"] for m in methods]
    execution_time = [results[m]["runtime"] for m in methods]

    # åˆ›å»ºå­å›¾
    fig, axs = plt.subplots(1, 3, figsize=(18, 6))

    # æƒé‡æ–¹å·®
    axs[0].bar(methods, weight_variance)
    axs[0].set_title("Weight Variance")
    axs[0].set_ylabel("Variance")

    # å½’ä¸€åŒ–åˆ‡è¾¹
    axs[1].bar(methods, edge_cut)
    axs[1].set_title("Normalized Edge Cut")
    axs[1].set_ylabel("Ratio")

    # æ‰§è¡Œæ—¶é—´
    axs[2].bar(methods, execution_time)
    axs[2].set_title("Execution Time")
    axs[2].set_ylabel("Seconds")    
    plt.suptitle(f"Algorithm Comparison - {graph_name}")
    plt.tight_layout()
    plt.savefig(f"{results_dir}/plots/{graph_name}_comparison.png")
    plt.savefig(f"{results_dir}/plots/{graph_name}_comparison.svg", format='svg')
    plt.close()


def main():
    # é™åˆ¶ PyTorch å’Œåº•å±‚åº“ä½¿ç”¨çš„çº¿ç¨‹æ•°ï¼Œå‡å°‘åå°çº¿ç¨‹ç©ºé—²ç­‰å¾…
    # å»ºè®®è®¾ç½®ä¸ºç‰©ç†æ ¸å¿ƒæ•°ï¼Œä¾‹å¦‚ 4 æˆ– 8ï¼Œæ ¹æ®æ‚¨çš„æœåŠ¡å™¨è°ƒæ•´
    num_threads = 8
    torch.set_num_threads(num_threads)
    os.environ['OMP_NUM_THREADS'] = str(num_threads)
    os.environ['MKL_NUM_THREADS'] = str(num_threads)
    print(f"é™åˆ¶ PyTorch/OMP/MKL çº¿ç¨‹æ•°ä¸º: {num_threads}")

    # æ·»åŠ è¿™ä¸¤è¡Œç¦ç”¨å¼ºåˆ¶åŒæ­¥
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    os.environ['CUDA_FORCE_PTX_JIT'] = '0'

    """ä¸»å‡½æ•°"""
    # === ä¿®æ”¹ï¼šåˆ›å»ºä»¥æ—¶é—´æˆ³å‘½åçš„ç»“æœç›®å½• ===
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"results/{timestamp}"
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs(results_dir, exist_ok=True)
    os.makedirs(f"{results_dir}/plots", exist_ok=True)
    os.makedirs(f"{results_dir}/models", exist_ok=True)
    
    print(f"è®­ç»ƒå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ç»“æœå°†ä¿å­˜åˆ°: {results_dir}")

    # é»˜è®¤é…ç½®
    default_config = {
        "episodes": 500,
        "max_steps": 100,
        "batch_size": 32,
        "methods": ["random", "greedy", "spectral", "metis", "dqn", "gnn", "ppo", "gnn_ppo"],
        "dqn_config": {
            "gamma": 0.95,
            "epsilon": 1.0,
            "epsilon_min": 0.01,
            "epsilon_decay": 0.995,
            "learning_rate": 0.001,
            "target_update_freq": 10
        },
        "gnn_config": {
            "gamma": 0.95,
            "epsilon": 1.0,
            "epsilon_min": 0.01,
            "epsilon_decay": 0.995,
            "learning_rate": 0.001,
            "hidden_dim": 128,
            "target_update_freq": 10
        },
        "ppo_config": {
            "gamma": 0.99,
            "learning_rate": 0.0003,
            "ppo_epochs": 4,
            "batch_size": 64,
            "clip_ratio": 0.2,
            "entropy_coef": 0.01,
            "value_coef": 0.5
        },
        "gnn_ppo_config": {
            "gamma": 0.99,
            "learning_rate": 0.0003,
            "ppo_epochs": 4,
            "batch_size": 64,
            "clip_ratio": 0.2,
            "entropy_coef": 0.01,
            "value_coef": 0.5,
            "hidden_dim": 128,
            "gnn_layers": 2
        }
    }

    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
    if not os.path.exists("configs/default.json"):
        with open("configs/default.json", "w") as f:
            json.dump(default_config, f, indent=4)

    # åŠ è½½é…ç½®
    config = load_config("configs/default.json")

    # === ä¿®æ”¹ï¼šä¼˜å…ˆå°è¯•åŠ è½½çœŸå®å›¾ï¼Œå¦åˆ™ä½¿ç”¨æµ‹è¯•å›¾ ===
    real_graph_path = "ctu_airspace_graph_1900_2000_kmeans.graphml"
    
    if os.path.exists(real_graph_path):
        print(f"ğŸ”„ ä½¿ç”¨çœŸå®ç©ºåŸŸå›¾: {real_graph_path}")
        try:
            graph = nx.read_graphml(real_graph_path)
            
            # é‡æ–°ç¼–å·èŠ‚ç‚¹ç¡®ä¿è¿ç»­æ€§
            node_mapping = {old_node: i for i, old_node in enumerate(graph.nodes())}
            graph = nx.relabel_nodes(graph, node_mapping)
            
            # ç¡®ä¿æƒé‡ä¸ºæ•°å€¼ç±»å‹
            for node in graph.nodes():
                if 'weight' in graph.nodes[node]:
                    graph.nodes[node]['weight'] = float(graph.nodes[node]['weight'])
                else:
                    graph.nodes[node]['weight'] = 1.0
            
            # å¦‚æœæ²¡æœ‰è¾¹ï¼Œæ·»åŠ ç®€å•çš„è¿æ¥
            if graph.number_of_edges() == 0:
                print("âš ï¸  å›¾æ²¡æœ‰è¾¹ï¼Œæ·»åŠ åŸºæœ¬è¿æ¥...")
                nodes = list(graph.nodes())
                for i in range(len(nodes) - 1):
                    graph.add_edge(nodes[i], nodes[i + 1])
                # æ·»åŠ ä¸€äº›éšæœºè¿æ¥
                import random
                for _ in range(min(50, len(nodes) * 2)):
                    u, v = random.choice(nodes), random.choice(nodes)
                    if u != v:
                        graph.add_edge(u, v)
            
            num_partitions = 3 if graph.number_of_nodes() > 15 else 2
            graph_name = f"real_airspace_{graph.number_of_nodes()}nodes"
            
            print(f"âœ… çœŸå®å›¾åŠ è½½æˆåŠŸ: {graph.number_of_nodes()}èŠ‚ç‚¹, {graph.number_of_edges()}è¾¹, {num_partitions}åˆ†åŒº")
            
        except Exception as e:
            print(f"âŒ çœŸå®å›¾åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°æµ‹è¯•å›¾...")
            graph = create_test_graph(num_nodes=10, seed=42)
            num_partitions = 2
            graph_name = "test_graph_10"
    else:
        print(f"ğŸ”„ çœŸå®å›¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨æµ‹è¯•å›¾")
        graph = create_test_graph(num_nodes=10, seed=42)
        num_partitions = 2
        graph_name = "test_graph_10"

    print("å¼€å§‹å›¾åˆ’åˆ†å®éªŒ...")
    df = run_experiment(graph_name, graph, num_partitions, config, results_dir)

    # === æ–°å¢ï¼šåˆ›å»ºè®­ç»ƒä¿¡æ¯è®°å½•æ–‡ä»¶ ===
    training_info = {
        "è®­ç»ƒå¼€å§‹æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "è®­ç»ƒå®Œæˆæ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "å›¾èŠ‚ç‚¹æ•°": len(graph.nodes()),
        "å›¾è¾¹æ•°": len(graph.edges()),
        "åˆ†åŒºæ•°": num_partitions,
        "è®­ç»ƒé…ç½®": config,
        "æœ€ç»ˆç»“æœ": df.to_dict('records')
    }
    
    import json
    with open(f"{results_dir}/training_info.json", "w", encoding='utf-8') as f:
        json.dump(training_info, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºç®€æ´çš„READMEæ–‡ä»¶
    readme_content = f"""# å›¾åˆ’åˆ†å®éªŒç»“æœ

## è®­ç»ƒä¿¡æ¯
- **å¼€å§‹æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **å›¾è§„æ¨¡**: {len(graph.nodes())} ä¸ªèŠ‚ç‚¹, {len(graph.edges())} æ¡è¾¹
- **åˆ†åŒºæ•°**: {num_partitions}
- **è®­ç»ƒæ–¹æ³•**: {', '.join(config.get('methods', []))}

## æ–‡ä»¶è¯´æ˜
- `plots/`: åŒ…å«æ‰€æœ‰è®­ç»ƒæ›²çº¿å’Œæ¯”è¾ƒå›¾è¡¨
- `models/`: åŒ…å«è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶
- `*.csv`: å®éªŒç»“æœæ•°æ®
- `training_info.json`: è¯¦ç»†çš„è®­ç»ƒé…ç½®å’Œç»“æœ

## æœ€ä½³ç»“æœé¢„è§ˆ
{df.to_string(index=False)}
"""
    
    with open(f"{results_dir}/README.md", "w", encoding='utf-8') as f:
        f.write(readme_content)

    print(f"\nå®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {results_dir} ç›®å½•")
    print(f"æŸ¥çœ‹ {results_dir}/README.md äº†è§£è¯¦ç»†ä¿¡æ¯")
    print(df)



if __name__ == "__main__":
    main()